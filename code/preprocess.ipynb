{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 13:22:45.641472: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>processed_title</th>\n",
       "      <th>ups</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1020ac</td>\n",
       "      <td>There's nothing inside / There is nothing outs...</td>\n",
       "      <td>5</td>\n",
       "      <td>[('inside', 0.5268), ('outside', 0.3751), ('se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>107cob</td>\n",
       "      <td>From whole we crumble / Forever lost to chaos ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[('chaos', 0.5962), ('crumble', 0.4749), ('for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>109a51</td>\n",
       "      <td>Indistinctiveness / Immeasurability / Capitalism</td>\n",
       "      <td>3</td>\n",
       "      <td>[('indistinctiveness', 0.7664), ('immeasurabil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10eysi</td>\n",
       "      <td>Internet is down / Obligations go bye-bye / Of...</td>\n",
       "      <td>9</td>\n",
       "      <td>[('office', 0.5033), ('obligations', 0.4663), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10f79k</td>\n",
       "      <td>Cotton in my mouth / Needles in my blood and b...</td>\n",
       "      <td>1</td>\n",
       "      <td>[('needles', 0.5314), ('cotton', 0.4806), ('bl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      id                                    processed_title  ups  \\\n",
       "0           0  1020ac  There's nothing inside / There is nothing outs...    5   \n",
       "1           1  107cob  From whole we crumble / Forever lost to chaos ...    1   \n",
       "2           2  109a51   Indistinctiveness / Immeasurability / Capitalism    3   \n",
       "3           3  10eysi  Internet is down / Obligations go bye-bye / Of...    9   \n",
       "4           4  10f79k  Cotton in my mouth / Needles in my blood and b...    1   \n",
       "\n",
       "                                            keywords  \n",
       "0  [('inside', 0.5268), ('outside', 0.3751), ('se...  \n",
       "1  [('chaos', 0.5962), ('crumble', 0.4749), ('for...  \n",
       "2  [('indistinctiveness', 0.7664), ('immeasurabil...  \n",
       "3  [('office', 0.5033), ('obligations', 0.4663), ...  \n",
       "4  [('needles', 0.5314), ('cotton', 0.4806), ('bl...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/haiku.csv')\n",
    "data = data.replace(\"/\", \" / \", regex=True)\n",
    "data = data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence: str):\n",
    "    return list(sentence.lower().split())\n",
    "\n",
    "def vectorize(tokens):\n",
    "    vocab, index = {}, 1  # start indexing from 1\n",
    "    vocab['<pad>'] = 0  # add a padding token\n",
    "    for token in tokens:\n",
    "        token = token.strip()\n",
    "        if token not in vocab:\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab\n",
    "\n",
    "def find_max_length(vectorized_poems):\n",
    "    max_length = 0\n",
    "    for poem in vectorized_poems:\n",
    "        # if len(poem)\n",
    "        max_length = max(max_length, len(poem))   \n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \" \".join(data[\"processed_title\"].to_list())\n",
    "tokens = tokenize(all_text)\n",
    "vocab_map = vectorize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>processed_title</th>\n",
       "      <th>ups</th>\n",
       "      <th>keywords</th>\n",
       "      <th>vectorized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1020ac</td>\n",
       "      <td>There's nothing inside / There is nothing outs...</td>\n",
       "      <td>5</td>\n",
       "      <td>[('inside', 0.5268), ('outside', 0.3751), ('se...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 2, 7, 8, 4, 9, 10, 11, 12, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>107cob</td>\n",
       "      <td>From whole we crumble / Forever lost to chaos ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[('chaos', 0.5962), ('crumble', 0.4749), ('for...</td>\n",
       "      <td>[14, 15, 16, 17, 4, 18, 19, 20, 21, 4, 22, 23,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>109a51</td>\n",
       "      <td>Indistinctiveness / Immeasurability / Capitalism</td>\n",
       "      <td>3</td>\n",
       "      <td>[('indistinctiveness', 0.7664), ('immeasurabil...</td>\n",
       "      <td>[25, 4, 26, 4, 27]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10eysi</td>\n",
       "      <td>Internet is down / Obligations go bye-bye / Of...</td>\n",
       "      <td>9</td>\n",
       "      <td>[('office', 0.5033), ('obligations', 0.4663), ...</td>\n",
       "      <td>[28, 6, 29, 4, 30, 31, 32, 4, 33, 34]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10f79k</td>\n",
       "      <td>Cotton in my mouth / Needles in my blood and b...</td>\n",
       "      <td>1</td>\n",
       "      <td>[('needles', 0.5314), ('cotton', 0.4806), ('bl...</td>\n",
       "      <td>[35, 12, 36, 37, 4, 38, 12, 36, 39, 40, 41, 4,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      id                                    processed_title  ups  \\\n",
       "0           0  1020ac  There's nothing inside / There is nothing outs...    5   \n",
       "1           1  107cob  From whole we crumble / Forever lost to chaos ...    1   \n",
       "2           2  109a51   Indistinctiveness / Immeasurability / Capitalism    3   \n",
       "3           3  10eysi  Internet is down / Obligations go bye-bye / Of...    9   \n",
       "4           4  10f79k  Cotton in my mouth / Needles in my blood and b...    1   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [('inside', 0.5268), ('outside', 0.3751), ('se...   \n",
       "1  [('chaos', 0.5962), ('crumble', 0.4749), ('for...   \n",
       "2  [('indistinctiveness', 0.7664), ('immeasurabil...   \n",
       "3  [('office', 0.5033), ('obligations', 0.4663), ...   \n",
       "4  [('needles', 0.5314), ('cotton', 0.4806), ('bl...   \n",
       "\n",
       "                                          vectorized  \n",
       "0  [1, 2, 3, 4, 5, 6, 2, 7, 8, 4, 9, 10, 11, 12, 13]  \n",
       "1  [14, 15, 16, 17, 4, 18, 19, 20, 21, 4, 22, 23,...  \n",
       "2                                 [25, 4, 26, 4, 27]  \n",
       "3              [28, 6, 29, 4, 30, 31, 32, 4, 33, 34]  \n",
       "4  [35, 12, 36, 37, 4, 38, 12, 36, 39, 40, 41, 4,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"vectorized\"] = data[\"processed_title\"].apply(lambda x: [vocab_map[t.strip()] for t in x.lower().split()])\n",
    "data = data[[a.count(4) <= 2 for a in data['vectorized']]]\n",
    "data = data[data['vectorized'].apply(lambda x: len(x) <= 19)]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vectorized_list = data[\"vectorized\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_data = pad_sequences(data_vectorized_list, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,     2,     3, ...,     0,     0,     0],\n",
       "       [   14,    15,    16, ...,     0,     0,     0],\n",
       "       [   25,     4,    26, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  572,  6718, 24145, ...,     0,     0,     0],\n",
       "       [ 1138,  7924,  1572, ...,     0,     0,     0],\n",
       "       [  422, 24147,     4, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haikus = np.array(padded_data)\n",
    "haikus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([1, 2, 3, 4, 5], dtype=int32), 6),\n",
       " (array([2, 3, 4, 5, 6], dtype=int32), 2),\n",
       " (array([3, 4, 5, 6, 2], dtype=int32), 7),\n",
       " (array([4, 5, 6, 2, 7], dtype=int32), 8),\n",
       " (array([5, 6, 2, 7, 8], dtype=int32), 4),\n",
       " (array([6, 2, 7, 8, 4], dtype=int32), 9),\n",
       " (array([2, 7, 8, 4, 9], dtype=int32), 10),\n",
       " (array([ 7,  8,  4,  9, 10], dtype=int32), 11),\n",
       " (array([ 8,  4,  9, 10, 11], dtype=int32), 12),\n",
       " (array([ 4,  9, 10, 11, 12], dtype=int32), 13),\n",
       " (array([ 9, 10, 11, 12, 13], dtype=int32), 0),\n",
       " (array([10, 11, 12, 13,  0], dtype=int32), 0),\n",
       " (array([11, 12, 13,  0,  0], dtype=int32), 0),\n",
       " (array([12, 13,  0,  0,  0], dtype=int32), 0),\n",
       " (array([14, 15, 16, 17,  4], dtype=int32), 18),\n",
       " (array([15, 16, 17,  4, 18], dtype=int32), 19),\n",
       " (array([16, 17,  4, 18, 19], dtype=int32), 20),\n",
       " (array([17,  4, 18, 19, 20], dtype=int32), 21),\n",
       " (array([ 4, 18, 19, 20, 21], dtype=int32), 4),\n",
       " (array([18, 19, 20, 21,  4], dtype=int32), 22)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_examples = []\n",
    "\n",
    "for haiku in haikus:\n",
    "    for i in range(len(haiku) - window_size + 1):\n",
    "        input_words = haiku[i:i+window_size-1]\n",
    "        output_word = haiku[i+window_size-1]\n",
    "        training_examples.append((input_words, output_word))\n",
    "training_examples[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense, SimpleRNN, GRU\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def masked_loss_function(y_true, y_pred):\n",
    "    mask = K.cast(K.not_equal(y_true, 0), K.floatx())\n",
    "    loss = K.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    masked_loss = loss * mask\n",
    "    return K.sum(masked_loss) / K.sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab_map)\n",
    "embedding_size = 128\n",
    "input_length = window_size - 1\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=input_length),\n",
    "    GRU(32),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "model.compile(loss=masked_loss_function, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([example[0] for example in training_examples[:10000]])\n",
    "y = np.array([example[1] for example in training_examples[:10000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "313/313 [==============================] - 19s 49ms/step - loss: 182.3383 - accuracy: 0.3192\n",
      "Epoch 2/30\n",
      "313/313 [==============================] - 15s 49ms/step - loss: 141.4477 - accuracy: 0.3412\n",
      "Epoch 3/30\n",
      "313/313 [==============================] - 15s 49ms/step - loss: 131.7017 - accuracy: 0.3828\n",
      "Epoch 4/30\n",
      "313/313 [==============================] - 16s 50ms/step - loss: 125.6802 - accuracy: 0.4050\n",
      "Epoch 5/30\n",
      "313/313 [==============================] - 16s 51ms/step - loss: 120.8209 - accuracy: 0.4186\n",
      "Epoch 6/30\n",
      "313/313 [==============================] - 16s 51ms/step - loss: 116.0848 - accuracy: 0.4325\n",
      "Epoch 7/30\n",
      "313/313 [==============================] - 16s 51ms/step - loss: 111.0834 - accuracy: 0.4389\n",
      "Epoch 8/30\n",
      "313/313 [==============================] - 16s 52ms/step - loss: 105.7915 - accuracy: 0.4531\n",
      "Epoch 9/30\n",
      "313/313 [==============================] - 16s 50ms/step - loss: 99.8849 - accuracy: 0.4644\n",
      "Epoch 10/30\n",
      "313/313 [==============================] - 16s 51ms/step - loss: 94.2500 - accuracy: 0.4743\n",
      "Epoch 11/30\n",
      "313/313 [==============================] - 16s 52ms/step - loss: 87.6800 - accuracy: 0.4895\n",
      "Epoch 12/30\n",
      "313/313 [==============================] - 16s 52ms/step - loss: 81.6074 - accuracy: 0.5053\n",
      "Epoch 13/30\n",
      "313/313 [==============================] - 16s 52ms/step - loss: 75.6570 - accuracy: 0.5166\n",
      "Epoch 14/30\n",
      "313/313 [==============================] - 17s 53ms/step - loss: 69.8063 - accuracy: 0.5347\n",
      "Epoch 15/30\n",
      "313/313 [==============================] - 17s 53ms/step - loss: 64.1266 - accuracy: 0.5456\n",
      "Epoch 16/30\n",
      "313/313 [==============================] - 17s 54ms/step - loss: 58.5494 - accuracy: 0.5651\n",
      "Epoch 17/30\n",
      "313/313 [==============================] - 16s 52ms/step - loss: 53.3051 - accuracy: 0.5902\n",
      "Epoch 18/30\n",
      "313/313 [==============================] - 16s 52ms/step - loss: 48.1519 - accuracy: 0.6203\n",
      "Epoch 19/30\n",
      "313/313 [==============================] - 16s 51ms/step - loss: 43.1444 - accuracy: 0.6466\n",
      "Epoch 20/30\n",
      "313/313 [==============================] - 16s 51ms/step - loss: 38.6336 - accuracy: 0.6769\n",
      "Epoch 21/30\n",
      "313/313 [==============================] - 16s 51ms/step - loss: 34.7289 - accuracy: 0.7044\n",
      "Epoch 22/30\n",
      "313/313 [==============================] - 16s 51ms/step - loss: 31.5312 - accuracy: 0.7266\n",
      "Epoch 23/30\n",
      "313/313 [==============================] - 16s 51ms/step - loss: 28.1566 - accuracy: 0.7499\n",
      "Epoch 24/30\n",
      "313/313 [==============================] - 16s 51ms/step - loss: 24.9427 - accuracy: 0.7743\n",
      "Epoch 25/30\n",
      "313/313 [==============================] - 16s 51ms/step - loss: 22.8749 - accuracy: 0.7939\n",
      "Epoch 26/30\n",
      "313/313 [==============================] - 16s 52ms/step - loss: 20.3997 - accuracy: 0.8189\n",
      "Epoch 27/30\n",
      "313/313 [==============================] - 16s 51ms/step - loss: 18.5384 - accuracy: 0.8342\n",
      "Epoch 28/30\n",
      "313/313 [==============================] - 16s 51ms/step - loss: 17.0301 - accuracy: 0.8466\n",
      "Epoch 29/30\n",
      "313/313 [==============================] - 16s 51ms/step - loss: 15.2657 - accuracy: 0.8614\n",
      "Epoch 30/30\n",
      "313/313 [==============================] - 16s 51ms/step - loss: 13.8249 - accuracy: 0.8761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd608dde890>"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, batch_size=32, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<pad>',\n",
       " 1: \"there's\",\n",
       " 2: 'nothing',\n",
       " 3: 'inside',\n",
       " 4: '/',\n",
       " 5: 'there',\n",
       " 6: 'is',\n",
       " 7: 'outside',\n",
       " 8: 'me',\n",
       " 9: 'i',\n",
       " 10: 'search',\n",
       " 11: 'on',\n",
       " 12: 'in',\n",
       " 13: 'hope.',\n",
       " 14: 'from',\n",
       " 15: 'whole',\n",
       " 16: 'we',\n",
       " 17: 'crumble',\n",
       " 18: 'forever',\n",
       " 19: 'lost',\n",
       " 20: 'to',\n",
       " 21: 'chaos',\n",
       " 22: 'never',\n",
       " 23: 'one',\n",
       " 24: 'again',\n",
       " 25: 'indistinctiveness',\n",
       " 26: 'immeasurability',\n",
       " 27: 'capitalism',\n",
       " 28: 'internet',\n",
       " 29: 'down',\n",
       " 30: 'obligations',\n",
       " 31: 'go',\n",
       " 32: 'bye-bye',\n",
       " 33: 'office',\n",
       " 34: 'rejoices',\n",
       " 35: 'cotton',\n",
       " 36: 'my',\n",
       " 37: 'mouth',\n",
       " 38: 'needles',\n",
       " 39: 'blood',\n",
       " 40: 'and',\n",
       " 41: 'bones',\n",
       " 42: 'hammers',\n",
       " 43: 'head.',\n",
       " 44: 'mighty',\n",
       " 45: 'hummingbird',\n",
       " 46: 'drinks',\n",
       " 47: 'a',\n",
       " 48: \"grapefruit's\",\n",
       " 49: 'blossom;',\n",
       " 50: 'blots',\n",
       " 51: 'out',\n",
       " 52: 'an',\n",
       " 53: 'airplane',\n",
       " 54: 'downvotes',\n",
       " 55: 'fall',\n",
       " 56: 'as',\n",
       " 57: 'sharp',\n",
       " 58: 'snowflakes',\n",
       " 59: 'of',\n",
       " 60: 'early',\n",
       " 61: 'winter,',\n",
       " 62: 'execution.',\n",
       " 63: 'seven',\n",
       " 64: 'ships',\n",
       " 65: 'tonight',\n",
       " 66: 'guess',\n",
       " 67: \"should've\",\n",
       " 68: 'said',\n",
       " 69: 'goodbye',\n",
       " 70: 'saw',\n",
       " 71: 'eight',\n",
       " 72: 'this',\n",
       " 73: 'morning.',\n",
       " 74: 'big',\n",
       " 75: 'words',\n",
       " 76: 'are',\n",
       " 77: 'so',\n",
       " 78: 'bad',\n",
       " 79: 'they',\n",
       " 80: 'can',\n",
       " 81: 'ruin',\n",
       " 82: 'haiku',\n",
       " 83: 'refrigerator',\n",
       " 84: 'at',\n",
       " 85: 'the',\n",
       " 86: 'end',\n",
       " 87: 'life',\n",
       " 88: 'kings,',\n",
       " 89: 'queens,',\n",
       " 90: 'pawns',\n",
       " 91: 'same',\n",
       " 92: 'box',\n",
       " 93: '\"free',\n",
       " 94: 'man\\'s\"',\n",
       " 95: 'short',\n",
       " 96: 'success',\n",
       " 97: 'measured',\n",
       " 98: 'money',\n",
       " 99: 'all',\n",
       " 100: 'work',\n",
       " 101: 'no',\n",
       " 102: 'play',\n",
       " 103: 'like',\n",
       " 104: 'lemmings',\n",
       " 105: 'off',\n",
       " 106: 'cliffs',\n",
       " 107: 'humans',\n",
       " 108: 'flock',\n",
       " 109: 'leader',\n",
       " 110: 'who',\n",
       " 111: 'just',\n",
       " 112: 'blind',\n",
       " 113: 'you',\n",
       " 114: 'left',\n",
       " 115: 'morning',\n",
       " 116: 'before',\n",
       " 117: 'woke.',\n",
       " 118: \"bed's\",\n",
       " 119: 'empty;',\n",
       " 120: 'toothbrush',\n",
       " 121: 'minty.',\n",
       " 122: 'toothbrush,',\n",
       " 123: 'world',\n",
       " 124: 'keeps',\n",
       " 125: 'spinning',\n",
       " 126: 'nobody',\n",
       " 127: 'notices',\n",
       " 128: 'but',\n",
       " 129: 'what',\n",
       " 130: 'if',\n",
       " 131: 'it',\n",
       " 132: 'stopped?',\n",
       " 133: 'live',\n",
       " 134: 'our',\n",
       " 135: 'lives',\n",
       " 136: 'make',\n",
       " 137: 'safely',\n",
       " 138: 'death;',\n",
       " 139: 'why',\n",
       " 140: 'not',\n",
       " 141: 'take',\n",
       " 142: 'some',\n",
       " 143: 'risks?',\n",
       " 144: 'endless',\n",
       " 145: 'beauty',\n",
       " 146: 'natures',\n",
       " 147: 'finest',\n",
       " 148: 'gift',\n",
       " 149: 'that',\n",
       " 150: 'girls',\n",
       " 151: 'shining',\n",
       " 152: 'grin.',\n",
       " 153: 'has',\n",
       " 154: 'mantra',\n",
       " 155: 'am',\n",
       " 156: 'sad',\n",
       " 157: 'discover',\n",
       " 158: 'fun',\n",
       " 159: 'over',\n",
       " 160: 'zombies',\n",
       " 161: 'real',\n",
       " 162: 'will',\n",
       " 163: 'break',\n",
       " 164: 'your',\n",
       " 165: 'fucking',\n",
       " 166: 'foot',\n",
       " 167: 'gimps',\n",
       " 168: 'slow',\n",
       " 169: 'hell',\n",
       " 170: 'buildings',\n",
       " 171: 'remain',\n",
       " 172: 'monuments',\n",
       " 173: 'excess',\n",
       " 174: 'be',\n",
       " 175: 'gone',\n",
       " 176: 'last',\n",
       " 177: 'drop',\n",
       " 178: 'oil',\n",
       " 179: 'burnt',\n",
       " 180: 'by',\n",
       " 181: 'tank,',\n",
       " 182: 'war',\n",
       " 183: 'autumn',\n",
       " 184: 'chill',\n",
       " 185: 'descends',\n",
       " 186: 'electric',\n",
       " 187: 'blanket',\n",
       " 188: 'returns',\n",
       " 189: 'snug',\n",
       " 190: 'cocoon',\n",
       " 191: 'sociopath',\n",
       " 192: 'pee',\n",
       " 193: 'trickle',\n",
       " 194: 'economics',\n",
       " 195: 'peer',\n",
       " 196: 'mirror',\n",
       " 197: 'fear',\n",
       " 198: 'face',\n",
       " 199: 'see',\n",
       " 200: 'broken',\n",
       " 201: 'yet',\n",
       " 202: 'beating.',\n",
       " 203: 'stand',\n",
       " 204: 'wall',\n",
       " 205: 'looking',\n",
       " 206: 'up',\n",
       " 207: 'cracks',\n",
       " 208: 'for',\n",
       " 209: 'signs',\n",
       " 210: 'future',\n",
       " 211: 'love',\n",
       " 212: 'light',\n",
       " 213: 'darkened',\n",
       " 214: 'may',\n",
       " 215: 'rest',\n",
       " 216: 'peace',\n",
       " 217: 'telltale',\n",
       " 218: 'arrive',\n",
       " 219: 'temper,',\n",
       " 220: 'sickness,',\n",
       " 221: 'mornings',\n",
       " 222: 'hello',\n",
       " 223: 'mom',\n",
       " 224: 'dad',\n",
       " 225: 'tune',\n",
       " 226: '(x-post',\n",
       " 227: 'r',\n",
       " 228: 'video)',\n",
       " 229: \"wouldn't\",\n",
       " 230: 'mind',\n",
       " 231: 'while',\n",
       " 232: 'floating',\n",
       " 233: 'waves',\n",
       " 234: 'swallows',\n",
       " 235: 'saccharine',\n",
       " 236: 'pop',\n",
       " 237: 'spills',\n",
       " 238: 'car',\n",
       " 239: 'stereo.',\n",
       " 240: \"it's\",\n",
       " 241: 'been',\n",
       " 242: 'long',\n",
       " 243: 'day.',\n",
       " 244: 'mist',\n",
       " 245: 'fades',\n",
       " 246: 'time',\n",
       " 247: \"sun'\",\n",
       " 248: 'best',\n",
       " 249: 'gold.',\n",
       " 250: 'her',\n",
       " 251: 'eyes',\n",
       " 252: 'kept',\n",
       " 253: 'westward.',\n",
       " 254: 'city',\n",
       " 255: 'limits',\n",
       " 256: 'turn',\n",
       " 257: 'signal',\n",
       " 258: 'keep',\n",
       " 259: 'driving.',\n",
       " 260: 'their',\n",
       " 261: 'kids',\n",
       " 262: 'could',\n",
       " 263: 'survive',\n",
       " 264: 'had',\n",
       " 265: 'stuff',\n",
       " 266: 'use',\n",
       " 267: 'wash',\n",
       " 268: 'suck',\n",
       " 269: 'haikus',\n",
       " 270: \"can't\",\n",
       " 271: 'think',\n",
       " 272: 'anything',\n",
       " 273: 'nature',\n",
       " 274: 'roars',\n",
       " 275: 'head',\n",
       " 276: 'die',\n",
       " 277: 'sheer',\n",
       " 278: 'terror',\n",
       " 279: 'those',\n",
       " 280: \"don't\",\n",
       " 281: 'have',\n",
       " 282: 'ran',\n",
       " 283: 'better',\n",
       " 284: 'do',\n",
       " 285: 'or',\n",
       " 286: 'regret',\n",
       " 287: 'having',\n",
       " 288: 'done',\n",
       " 289: 'clouded',\n",
       " 290: 'tears',\n",
       " 291: 'seeing',\n",
       " 292: 'freshly',\n",
       " 293: 'dug',\n",
       " 294: 'grave',\n",
       " 295: 'beneath',\n",
       " 296: 'cover',\n",
       " 297: 'howl',\n",
       " 298: 'other',\n",
       " 299: 'poems',\n",
       " 300: 'coin-sized',\n",
       " 301: 'spider',\n",
       " 302: 'birth',\n",
       " 303: 'moment',\n",
       " 304: 'death',\n",
       " 305: 'well',\n",
       " 306: 'fails',\n",
       " 307: 'torches',\n",
       " 308: 'concrete',\n",
       " 309: 'caverns',\n",
       " 310: 'sky',\n",
       " 311: \"haven't\",\n",
       " 312: 'changed',\n",
       " 313: 'much.',\n",
       " 314: 'upvotes',\n",
       " 315: 'increase',\n",
       " 316: 'wit',\n",
       " 317: 'comments',\n",
       " 318: 'decrease',\n",
       " 319: 'proportionately',\n",
       " 320: 'consolation',\n",
       " 321: 'prize',\n",
       " 322: 'goes',\n",
       " 323: 'ladies',\n",
       " 324: 'barbecue',\n",
       " 325: 'booth,',\n",
       " 326: 'hooray!',\n",
       " 327: 'vacancy',\n",
       " 328: 'heart',\n",
       " 329: 'hotel',\n",
       " 330: 'rented',\n",
       " 331: 'rooms.',\n",
       " 332: 'pen',\n",
       " 333: 'lifted',\n",
       " 334: \"creation's\",\n",
       " 335: 'flow',\n",
       " 336: 'ceases',\n",
       " 337: 'ink',\n",
       " 338: 'meet',\n",
       " 339: 'bar',\n",
       " 340: 'bodies',\n",
       " 341: 'coalesce',\n",
       " 342: 'breakfast',\n",
       " 343: 'after',\n",
       " 344: 'dawn',\n",
       " 345: 'man',\n",
       " 346: 'paints',\n",
       " 347: 'streetlight',\n",
       " 348: 'lone',\n",
       " 349: 'star',\n",
       " 350: 'party',\n",
       " 351: 'bus',\n",
       " 352: 'austin',\n",
       " 353: 'exhausting',\n",
       " 354: 'put',\n",
       " 355: 'lime',\n",
       " 356: 'coconut',\n",
       " 357: 'fix',\n",
       " 358: 'belly',\n",
       " 359: 'ache',\n",
       " 360: ',',\n",
       " 361: 'put...',\n",
       " 362: 'clanks',\n",
       " 363: 'coiled',\n",
       " 364: 'wire',\n",
       " 365: 'dodge',\n",
       " 366: 'deep',\n",
       " 367: 'holes',\n",
       " 368: 'iced',\n",
       " 369: 'pavement',\n",
       " 370: 'very',\n",
       " 371: 'blown',\n",
       " 372: 'shock',\n",
       " 373: 'sweat',\n",
       " 374: 'everyday',\n",
       " 375: 'focus',\n",
       " 376: 'rises',\n",
       " 377: 'above',\n",
       " 378: 'conquer',\n",
       " 379: 'everything.',\n",
       " 380: 'sitting',\n",
       " 381: 'desk',\n",
       " 382: 'butt',\n",
       " 383: 'starting',\n",
       " 384: 'sleep',\n",
       " 385: 'hope',\n",
       " 386: \"doesn't\",\n",
       " 387: 'snore',\n",
       " 388: 'stupid',\n",
       " 389: 'flattened',\n",
       " 390: 'tire.',\n",
       " 391: 'curse',\n",
       " 392: 'frigid',\n",
       " 393: 'night,',\n",
       " 394: 'wait',\n",
       " 395: 'caa.',\n",
       " 396: 'op',\n",
       " 397: 'fag',\n",
       " 398: 'bro,',\n",
       " 399: 'he',\n",
       " 400: 'even',\n",
       " 401: 'lift',\n",
       " 402: 'jimmies',\n",
       " 403: 'rustled',\n",
       " 404: 'making',\n",
       " 405: 'deal',\n",
       " 406: 'with',\n",
       " 407: 'it,',\n",
       " 408: 'mother',\n",
       " 409: 'fucker',\n",
       " 410: 'socks,',\n",
       " 411: 'smell',\n",
       " 412: 'bleach',\n",
       " 413: 'laundry',\n",
       " 414: 'week',\n",
       " 415: 'filthy,',\n",
       " 416: 'redtube',\n",
       " 417: 'socks',\n",
       " 418: 'chilling',\n",
       " 419: 'homies',\n",
       " 420: 'need',\n",
       " 421: 'watermelon',\n",
       " 422: 'now',\n",
       " 423: 'used',\n",
       " 424: 'welfare',\n",
       " 425: 'ornithology',\n",
       " 426: 'wave',\n",
       " 427: 'behaviors',\n",
       " 428: 'glorious',\n",
       " 429: '\"how\\'d',\n",
       " 430: 'spell',\n",
       " 431: '\\'haiku?\\'\"',\n",
       " 432: '\"it\\'s,',\n",
       " 433: 'uh,',\n",
       " 434: 'h-a-i-k-u\"',\n",
       " 435: '\"gee,',\n",
       " 436: 'thanks',\n",
       " 437: 'help.\"',\n",
       " 438: 'electricity',\n",
       " 439: 'flashing',\n",
       " 440: 'across',\n",
       " 441: 'night',\n",
       " 442: 'sky;',\n",
       " 443: 'illuminating',\n",
       " 444: 'dream',\n",
       " 445: 'god',\n",
       " 446: 'know',\n",
       " 447: 'ending',\n",
       " 448: 'none',\n",
       " 449: 'hear',\n",
       " 450: 'golden',\n",
       " 451: 'flashes',\n",
       " 452: 'emptiness',\n",
       " 453: 'darkness',\n",
       " 454: 'consumes',\n",
       " 455: 'shines',\n",
       " 456: 'afar',\n",
       " 457: 'pain,',\n",
       " 458: 'freedom,',\n",
       " 459: 'death,',\n",
       " 460: 'happiness',\n",
       " 461: 'dead',\n",
       " 462: 'flies,',\n",
       " 463: 'pleasure,',\n",
       " 464: 'sacrifice',\n",
       " 465: 'rubber',\n",
       " 466: 'ducky',\n",
       " 467: 'bitch!',\n",
       " 468: 'first,',\n",
       " 469: \"bachelor's\",\n",
       " 470: 'then,',\n",
       " 471: 'm.b.a.',\n",
       " 472: 'j.d.',\n",
       " 473: \"where's\",\n",
       " 474: 'job?',\n",
       " 475: 'celebrities',\n",
       " 476: 'say',\n",
       " 477: 'realize',\n",
       " 478: \"heaven's\",\n",
       " 479: 'blue',\n",
       " 480: 'vault',\n",
       " 481: 'crowded',\n",
       " 482: 'opulent',\n",
       " 483: 'clouds',\n",
       " 484: 'kites',\n",
       " 485: 'kami.',\n",
       " 486: 'bridge',\n",
       " 487: 'vast',\n",
       " 488: 'crossing',\n",
       " 489: 'gaps',\n",
       " 490: 'joining',\n",
       " 491: 'friends',\n",
       " 492: 'worldly',\n",
       " 493: 'within.',\n",
       " 494: 'yay!',\n",
       " 495: 'new',\n",
       " 496: 'subreddit',\n",
       " 497: 'funny!',\n",
       " 498: 'damn.',\n",
       " 499: 'all.',\n",
       " 500: 'dies',\n",
       " 501: 'eighty',\n",
       " 502: 'dropped',\n",
       " 503: 'soap',\n",
       " 504: 'prison',\n",
       " 505: 'shower',\n",
       " 506: 'was',\n",
       " 507: 'brutally',\n",
       " 508: 'stabbed',\n",
       " 509: 'migrating',\n",
       " 510: 'birds',\n",
       " 511: \"water's\",\n",
       " 512: 'green',\n",
       " 513: 'sloped',\n",
       " 514: 'shore.',\n",
       " 515: 'water',\n",
       " 516: 'treatment',\n",
       " 517: 'plant.',\n",
       " 518: 'wrote',\n",
       " 519: 'haiku,',\n",
       " 520: \"didn't\",\n",
       " 521: 'enjoy',\n",
       " 522: 'much,',\n",
       " 523: 'threw',\n",
       " 524: 'out.',\n",
       " 525: 'spherical',\n",
       " 526: 'shape',\n",
       " 527: \"sun's\",\n",
       " 528: 'bright',\n",
       " 529: 'essence',\n",
       " 530: 'contained',\n",
       " 531: 'within',\n",
       " 532: 'white',\n",
       " 533: 'orb',\n",
       " 534: \"i'm\",\n",
       " 535: 'around',\n",
       " 536: 'here',\n",
       " 537: 'seem',\n",
       " 538: 'nice',\n",
       " 539: 'people',\n",
       " 540: 'damn,',\n",
       " 541: 'hard',\n",
       " 542: 'claw',\n",
       " 543: 'locked',\n",
       " 544: 'beak',\n",
       " 545: 'crushed',\n",
       " 546: 'landslide',\n",
       " 547: 'fighting',\n",
       " 548: '575',\n",
       " 549: 'peterson',\n",
       " 550: 'free',\n",
       " 551: 'through',\n",
       " 552: '12',\n",
       " 553: '21',\n",
       " 554: '(kindle',\n",
       " 555: 'edition)',\n",
       " 556: '-',\n",
       " 557: 'please',\n",
       " 558: 'share',\n",
       " 559: 'thoughts',\n",
       " 560: 'should',\n",
       " 561: 'afraid',\n",
       " 562: 'going',\n",
       " 563: 'back',\n",
       " 564: 'regularly',\n",
       " 565: 'scheduled',\n",
       " 566: 'programming',\n",
       " 567: 'walked',\n",
       " 568: 'field',\n",
       " 569: 'went',\n",
       " 570: 'along',\n",
       " 571: 'back?',\n",
       " 572: 'oh',\n",
       " 573: 'how',\n",
       " 574: 'dreams',\n",
       " 575: 'haunt',\n",
       " 576: 'show',\n",
       " 577: 'false',\n",
       " 578: 'reality',\n",
       " 579: 'lived',\n",
       " 580: 'fake',\n",
       " 581: 'parting',\n",
       " 582: 'ways',\n",
       " 583: 'bittersweet',\n",
       " 584: 'still',\n",
       " 585: 'beats',\n",
       " 586: 'confusing',\n",
       " 587: 'felt',\n",
       " 588: 'warm',\n",
       " 589: 'summer',\n",
       " 590: 'breeze',\n",
       " 591: 'sparkling',\n",
       " 592: 'air',\n",
       " 593: 'soul',\n",
       " 594: 'captured',\n",
       " 595: 'flight',\n",
       " 596: 'cloud',\n",
       " 597: 'two',\n",
       " 598: 'companions',\n",
       " 599: 'artists',\n",
       " 600: 'warcraft',\n",
       " 601: 'trick',\n",
       " 602: 'frozen',\n",
       " 603: 'gift,',\n",
       " 604: 'running,',\n",
       " 605: 'bye',\n",
       " 606: 'never-ending',\n",
       " 607: 'kite.',\n",
       " 608: 'rescued',\n",
       " 609: 'him',\n",
       " 610: 'she',\n",
       " 611: 'needs',\n",
       " 612: 'rescuing',\n",
       " 613: 'me.',\n",
       " 614: 'perhaps',\n",
       " 615: 'window',\n",
       " 616: 'fly',\n",
       " 617: 'paper',\n",
       " 618: 'sure,',\n",
       " 619: 'covetous',\n",
       " 620: 'hands',\n",
       " 621: 'unhinge',\n",
       " 622: 'topmost',\n",
       " 623: 'button',\n",
       " 624: 'blouse',\n",
       " 625: 'sing',\n",
       " 626: 'song',\n",
       " 627: 'uplifting',\n",
       " 628: \"world's\",\n",
       " 629: 'darkness.',\n",
       " 630: 'falls',\n",
       " 631: 'winter',\n",
       " 632: 'silent',\n",
       " 633: 'snow',\n",
       " 634: 'canvas',\n",
       " 635: 'black',\n",
       " 636: 'birdsong',\n",
       " 637: 'bursting',\n",
       " 638: 'forth',\n",
       " 639: 'hotboxed',\n",
       " 640: 'five-seater',\n",
       " 641: 'bullshitting',\n",
       " 642: 'stranger',\n",
       " 643: 'infatuation.',\n",
       " 644: 'sun',\n",
       " 645: 'look',\n",
       " 646: 'wonders',\n",
       " 647: 'grateful',\n",
       " 648: 'today',\n",
       " 649: 'sexy,',\n",
       " 650: 'spunky',\n",
       " 651: 'girl',\n",
       " 652: 'tolerates',\n",
       " 653: 'silliness.',\n",
       " 654: 'means',\n",
       " 655: '[#3]',\n",
       " 656: 'unwinds',\n",
       " 657: 'help',\n",
       " 658: 'wonder',\n",
       " 659: \"you're\",\n",
       " 660: 'stars.',\n",
       " 661: '[4]i',\n",
       " 662: 'lie',\n",
       " 663: 'speechless',\n",
       " 664: 'gaze',\n",
       " 665: 'fixed',\n",
       " 666: 'upon',\n",
       " 667: 'heavens',\n",
       " 668: 'beckons',\n",
       " 669: '[#5]',\n",
       " 670: 'open',\n",
       " 671: 'pain',\n",
       " 672: 'away',\n",
       " 673: 'let',\n",
       " 674: 'honey',\n",
       " 675: 'bucket',\n",
       " 676: 'sits,',\n",
       " 677: 'awaiting',\n",
       " 678: 'excrement',\n",
       " 679: 'falling',\n",
       " 680: 'rain.',\n",
       " 681: 'brings',\n",
       " 682: 'social',\n",
       " 683: 'shyness',\n",
       " 684: 'where',\n",
       " 685: 'orpheus',\n",
       " 686: 'runs',\n",
       " 687: 'sober',\n",
       " 688: 'overcast',\n",
       " 689: 'clear',\n",
       " 690: 'stuck',\n",
       " 691: 'behind',\n",
       " 692: 'glass',\n",
       " 693: 'practice',\n",
       " 694: 'more',\n",
       " 695: 'become',\n",
       " 696: 'awesome.',\n",
       " 697: 'would',\n",
       " 698: 'escape',\n",
       " 699: 'weiner,',\n",
       " 700: 'pants',\n",
       " 701: 'case',\n",
       " 702: 'fire',\n",
       " 703: 'sleeping',\n",
       " 704: 'drug',\n",
       " 705: 'bed',\n",
       " 706: 'being',\n",
       " 707: 'dealer',\n",
       " 708: 'alarm',\n",
       " 709: 'law',\n",
       " 710: 'cooked',\n",
       " 711: 'pizza',\n",
       " 712: 'cut',\n",
       " 713: 'bastard',\n",
       " 714: 'into',\n",
       " 715: 'squares',\n",
       " 716: 'ate',\n",
       " 717: 'corners',\n",
       " 718: 'asks',\n",
       " 719: 'says',\n",
       " 720: 'stay',\n",
       " 721: 'delete',\n",
       " 722: 'number',\n",
       " 723: 'wedding',\n",
       " 724: 'bells',\n",
       " 725: 'ring',\n",
       " 726: 'dove',\n",
       " 727: 'released',\n",
       " 728: 'cage',\n",
       " 729: \"man's\",\n",
       " 730: 'wilted',\n",
       " 731: 'chirp',\n",
       " 732: 'sweetly',\n",
       " 733: 'trees',\n",
       " 734: 'wish',\n",
       " 735: 'learn',\n",
       " 736: 'state',\n",
       " 737: 'knuckles',\n",
       " 738: 'tell',\n",
       " 739: 'lot',\n",
       " 740: 'menace',\n",
       " 741: 'south',\n",
       " 742: 'central',\n",
       " 743: 'drinking',\n",
       " 744: 'juice',\n",
       " 745: 'hood.',\n",
       " 746: 'started',\n",
       " 747: 'five',\n",
       " 748: 'swiftly',\n",
       " 749: 'came',\n",
       " 750: 'curvy',\n",
       " 751: 'windy',\n",
       " 752: 'road',\n",
       " 753: 'seat',\n",
       " 754: 'belt',\n",
       " 755: 'against',\n",
       " 756: 'neck',\n",
       " 757: '\"mom',\n",
       " 758: 'yet?\"',\n",
       " 759: 'entombed',\n",
       " 760: 'thick',\n",
       " 761: 'whines',\n",
       " 762: 'does',\n",
       " 763: 'move',\n",
       " 764: 'icy',\n",
       " 765: 'write',\n",
       " 766: 'myself',\n",
       " 767: 'ponder',\n",
       " 768: 'petulance',\n",
       " 769: 'laserbeams',\n",
       " 770: 'its',\n",
       " 771: 'called',\n",
       " 772: 'scifaiku',\n",
       " 773: 'were',\n",
       " 774: 'amazing',\n",
       " 775: 'seventy',\n",
       " 776: 'years',\n",
       " 777: 'give',\n",
       " 778: 'loves',\n",
       " 779: 'most',\n",
       " 780: 'embrace',\n",
       " 781: 'fits',\n",
       " 782: 'ever',\n",
       " 783: 'changing',\n",
       " 784: 'quickly.',\n",
       " 785: 'word.',\n",
       " 786: 'flowing',\n",
       " 787: 'waters',\n",
       " 788: 'valley',\n",
       " 789: 'kool-aid',\n",
       " 790: 'yes',\n",
       " 791: 'farming',\n",
       " 792: 'much',\n",
       " 793: 'harder',\n",
       " 794: 'than',\n",
       " 795: 'thought',\n",
       " 796: 'nemesis',\n",
       " 797: 'panini',\n",
       " 798: 'floor',\n",
       " 799: \"dog's\",\n",
       " 800: 'happy,',\n",
       " 801: 'least',\n",
       " 802: 'days',\n",
       " 803: 'beach',\n",
       " 804: 'sea',\n",
       " 805: 'grass',\n",
       " 806: 'shore',\n",
       " 807: 'mermaid',\n",
       " 808: 'born',\n",
       " 809: 'when',\n",
       " 810: 'kissed',\n",
       " 811: 'died',\n",
       " 812: 'whilst',\n",
       " 813: 'loved',\n",
       " 814: 'meow',\n",
       " 815: 'bark',\n",
       " 816: 'lasts',\n",
       " 817: 'souvenirs',\n",
       " 818: 'sand',\n",
       " 819: 'spines',\n",
       " 820: 'books.',\n",
       " 821: 'shattered',\n",
       " 822: 'self',\n",
       " 823: 'esteem',\n",
       " 824: 'did',\n",
       " 825: 'get',\n",
       " 826: 'attached',\n",
       " 827: 'wrong?',\n",
       " 828: 'drift',\n",
       " 829: 'far',\n",
       " 830: 'wading',\n",
       " 831: 'empty',\n",
       " 832: 'drought',\n",
       " 833: 'galaxy',\n",
       " 834: 'beyond',\n",
       " 835: \"horizon's\",\n",
       " 836: 'edge',\n",
       " 837: 'always',\n",
       " 838: 'reach',\n",
       " 839: 'obscure',\n",
       " 840: 'entity',\n",
       " 841: 'indicates',\n",
       " 842: 'existence',\n",
       " 843: 'consuming',\n",
       " 844: 'way,',\n",
       " 845: 'way',\n",
       " 846: 'high',\n",
       " 847: 'feel',\n",
       " 848: 'precious',\n",
       " 849: 'dumbass',\n",
       " 850: 'manager',\n",
       " 851: 'fixes',\n",
       " 852: 'wasted',\n",
       " 853: 'joy!',\n",
       " 854: 'spring',\n",
       " 855: 'cometh',\n",
       " 856: 'sunset',\n",
       " 857: 'minute',\n",
       " 858: 'blog',\n",
       " 859: 'written',\n",
       " 860: 'waiting',\n",
       " 861: 'traffic',\n",
       " 862: 'markers',\n",
       " 863: 'special',\n",
       " 864: 'thing',\n",
       " 865: 'moon',\n",
       " 866: 'hides',\n",
       " 867: 'dark',\n",
       " 868: \"we're\",\n",
       " 869: 'slaves',\n",
       " 870: 'bored',\n",
       " 871: 'reddit',\n",
       " 872: 'offers',\n",
       " 873: 'solace',\n",
       " 874: 'kittens',\n",
       " 875: 'titties',\n",
       " 876: 'captivated',\n",
       " 877: 'smile',\n",
       " 878: 'calls',\n",
       " 879: 'facebook',\n",
       " 880: 'list',\n",
       " 881: 'purge',\n",
       " 882: 'similar',\n",
       " 883: 'murder',\n",
       " 884: 'digital',\n",
       " 885: 'hatchet',\n",
       " 886: 'petal',\n",
       " 887: 'departs',\n",
       " 888: 'dancing,',\n",
       " 889: 'patches',\n",
       " 890: 'wind',\n",
       " 891: 'cuts',\n",
       " 892: 'crab',\n",
       " 893: 'tuna',\n",
       " 894: 'salmon',\n",
       " 895: 'sweet',\n",
       " 896: 'potato',\n",
       " 897: 'waffle',\n",
       " 898: 'fries',\n",
       " 899: 'pinot',\n",
       " 900: 'grigio',\n",
       " 901: 'blissful',\n",
       " 902: 'misery',\n",
       " 903: 'beer',\n",
       " 904: 'cigarettes',\n",
       " 905: 'alone',\n",
       " 906: 'many',\n",
       " 907: 'vengeance',\n",
       " 908: 'sandwich',\n",
       " 909: '\"dad,',\n",
       " 910: 'there?\"',\n",
       " 911: 'eating',\n",
       " 912: 'pet',\n",
       " 913: 'truth',\n",
       " 914: 'wanted',\n",
       " 915: 'side',\n",
       " 916: 'deus',\n",
       " 917: 'ex',\n",
       " 918: 'machina',\n",
       " 919: '(am',\n",
       " 920: 'pronouncing',\n",
       " 921: 'right?)',\n",
       " 922: 'only',\n",
       " 923: 'miss',\n",
       " 924: 'friendship',\n",
       " 925: 'meant',\n",
       " 926: 'sail',\n",
       " 927: 'docks',\n",
       " 928: 'squeezing',\n",
       " 929: 'agony',\n",
       " 930: 'splashes,',\n",
       " 931: 'red',\n",
       " 932: 'faced',\n",
       " 933: 'relief',\n",
       " 934: 'star-pricked',\n",
       " 935: 'past',\n",
       " 936: 'snow-salted',\n",
       " 937: 'woods',\n",
       " 938: 'clanging',\n",
       " 939: 'cabins',\n",
       " 940: 'remains',\n",
       " 941: 'point',\n",
       " 942: 'without',\n",
       " 943: 'tried',\n",
       " 944: 'guys',\n",
       " 945: 'chomp',\n",
       " 946: 'chew',\n",
       " 947: 'swallow',\n",
       " 948: 'silence',\n",
       " 949: 'heard',\n",
       " 950: 'gulp',\n",
       " 951: 'dissapoint',\n",
       " 952: 'finally',\n",
       " 953: 'cats',\n",
       " 954: \"i'd\",\n",
       " 955: 'knead',\n",
       " 956: 'doughy',\n",
       " 957: 'butterface',\n",
       " 958: 'multigrain',\n",
       " 959: 'loaf',\n",
       " 960: 'touched',\n",
       " 961: 'breathing',\n",
       " 962: 'religious',\n",
       " 963: 'park',\n",
       " 964: 'want',\n",
       " 965: 'handicap',\n",
       " 966: 'unrequited',\n",
       " 967: 'really',\n",
       " 968: 'call',\n",
       " 969: 'that?',\n",
       " 970: 'lust',\n",
       " 971: 'emotionlessness',\n",
       " 972: 'void',\n",
       " 973: 'caused',\n",
       " 974: 'losing',\n",
       " 975: 'reflecting',\n",
       " 976: 'despair',\n",
       " 977: 'doing',\n",
       " 978: 'shaving?',\n",
       " 979: 'leave',\n",
       " 980: 'mass',\n",
       " 981: 'pubes.',\n",
       " 982: 'nasty!',\n",
       " 983: 'fertile',\n",
       " 984: 'ground',\n",
       " 985: 'nope!',\n",
       " 986: 'chuck',\n",
       " 987: 'testa',\n",
       " 988: 'soy',\n",
       " 989: 'un',\n",
       " 990: 'perdedor',\n",
       " 991: 'loser',\n",
       " 992: 'baby,',\n",
       " 993: 'kill',\n",
       " 994: 'me?',\n",
       " 995: 'ideas',\n",
       " 996: 'overwhelming',\n",
       " 997: 'page',\n",
       " 998: 'shapes',\n",
       " 999: 'laying',\n",
       " ...}"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_map_inv = dict([(value, key) for key, value in vocab_map.items()])\n",
    "vocab_map_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poem(input_words):\n",
    "    vectorized_input = [vocab_map[word] for word in input_words]\n",
    "    print(f\"User specified words {input_words} which were vectorized as {vectorized_input}\")\n",
    "    output_poem = input_words\n",
    "    for i in range(19 - window_size - 1):\n",
    "        input = np.array(vectorized_input[i:i+window_size -1]).reshape((1,window_size -1))\n",
    "        prediction = np.array(model.predict(input, verbose=0))\n",
    "        new_word_vector = (prediction[0].argsort()[::-1])[np.random.randint(0, 1)]\n",
    "        vectorized_input.append(new_word_vector)\n",
    "        new_word = vocab_map_inv[new_word_vector]\n",
    "        output_poem.append(new_word)\n",
    "    output = \" \".join(output_poem)\n",
    "    print(f\"OUTPUT POEM: {output}\")\n",
    "\n",
    "        \n",
    "                                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User specified words ['internet', 'is', 'down', 'and', 'i'] which were vectorized as [28, 6, 29, 40, 9]\n",
      "OUTPUT POEM: internet is down and i / pure war over it <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "generate_poem([\"internet\", \"is\", \"down\", \"and\", \"i\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User specified words ['fun', 'today', 'and', 'tomorrow', 'together'] which were vectorized as [158, 648, 40, 1660, 2213]\n",
      "OUTPUT POEM: fun today and tomorrow together / when i had that eat my dashboard / and into the\n"
     ]
    }
   ],
   "source": [
    "generate_poem([\"fun\", \"today\", \"and\", \"tomorrow\", \"together\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User specified words ['the', 'bear', 'and', 'i', 'cherry'] which were vectorized as [85, 1014, 40, 9, 1264]\n",
      "OUTPUT POEM: the bear and i cherry / tiny it's crickets / at your daily rest of me. <pad>\n"
     ]
    }
   ],
   "source": [
    "generate_poem([\"the\", \"bear\", \"and\", \"i\", \"cherry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User specified words ['i', 'am', 'coming', 'to', 'a'] which were vectorized as [9, 155, 2353, 20, 47]\n",
      "OUTPUT POEM: i am coming to a worlds <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "generate_poem([\"i\", \"am\", \"coming\", \"to\", \"a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_random_words(vocab_map, number):\n",
    "    vocabs = list(vocab_map.keys())\n",
    "    inds = list(np.random.randint(0, len(vocabs), number))\n",
    "    output_words = []\n",
    "    for i in inds:\n",
    "        output_words.append(vocabs[i])\n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User specified words ['gnarled,', 'claiming', 'rasping', 'caffine', 'limbs'] which were vectorized as [4414, 11891, 12004, 11931, 5376]\n",
      "OUTPUT POEM: gnarled, claiming rasping caffine limbs / under a gift, running, bye / never-ending oceans / pretending that\n"
     ]
    }
   ],
   "source": [
    "generate_poem(pick_random_words(vocab_map, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User specified words ['squeeze', 'mask', 'tree-', 'loves!', 'routine'] which were vectorized as [13581, 3258, 21235, 19684, 6553]\n",
      "OUTPUT POEM: squeeze mask tree- loves! routine / congealed the false reality / then whiskey calls to you <pad>\n"
     ]
    }
   ],
   "source": [
    "generate_poem(pick_random_words(vocab_map, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci1470",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
