{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>processed_title</th>\n",
       "      <th>ups</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1020ac</td>\n",
       "      <td>There's nothing inside / There is nothing outs...</td>\n",
       "      <td>5</td>\n",
       "      <td>[('inside', 0.5268), ('outside', 0.3751), ('se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>107cob</td>\n",
       "      <td>From whole we crumble / Forever lost to chaos ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[('chaos', 0.5962), ('crumble', 0.4749), ('for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>109a51</td>\n",
       "      <td>Indistinctiveness / Immeasurability / Capitalism</td>\n",
       "      <td>3</td>\n",
       "      <td>[('indistinctiveness', 0.7664), ('immeasurabil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10eysi</td>\n",
       "      <td>Internet is down / Obligations go bye-bye / Of...</td>\n",
       "      <td>9</td>\n",
       "      <td>[('office', 0.5033), ('obligations', 0.4663), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10f79k</td>\n",
       "      <td>Cotton in my mouth / Needles in my blood and b...</td>\n",
       "      <td>1</td>\n",
       "      <td>[('needles', 0.5314), ('cotton', 0.4806), ('bl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      id                                    processed_title  ups  \\\n",
       "0           0  1020ac  There's nothing inside / There is nothing outs...    5   \n",
       "1           1  107cob  From whole we crumble / Forever lost to chaos ...    1   \n",
       "2           2  109a51   Indistinctiveness / Immeasurability / Capitalism    3   \n",
       "3           3  10eysi  Internet is down / Obligations go bye-bye / Of...    9   \n",
       "4           4  10f79k  Cotton in my mouth / Needles in my blood and b...    1   \n",
       "\n",
       "                                            keywords  \n",
       "0  [('inside', 0.5268), ('outside', 0.3751), ('se...  \n",
       "1  [('chaos', 0.5962), ('crumble', 0.4749), ('for...  \n",
       "2  [('indistinctiveness', 0.7664), ('immeasurabil...  \n",
       "3  [('office', 0.5033), ('obligations', 0.4663), ...  \n",
       "4  [('needles', 0.5314), ('cotton', 0.4806), ('bl...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/haiku.csv')\n",
    "data = data.replace(\"/\", \" / \", regex=True)\n",
    "data = data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence: str):\n",
    "    tokens = sentence.lower().split()\n",
    "    tokens = [process_token(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "def process_token(token: str):\n",
    "    if token.strip() == \"/\":\n",
    "        return token\n",
    "    return re.sub(r'[^\\w\\s]', '', token.strip())\n",
    "\n",
    "def vectorize(tokens):\n",
    "    vocab, index = {}, 1\n",
    "    vocab['<pad>'] = 0\n",
    "    for token in tokens:\n",
    "        token = token.strip()\n",
    "        if token not in vocab:\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab\n",
    "\n",
    "def find_max_length(vectorized_poems):\n",
    "    max_length = 0\n",
    "    for poem in vectorized_poems:\n",
    "        max_length = max(max_length, len(poem))   \n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \" \".join(data[\"processed_title\"].to_list())\n",
    "tokens = tokenize(all_text)\n",
    "vocab_map = vectorize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " 'theres': 1,\n",
       " 'nothing': 2,\n",
       " 'inside': 3,\n",
       " '/': 4,\n",
       " 'there': 5,\n",
       " 'is': 6,\n",
       " 'outside': 7,\n",
       " 'me': 8,\n",
       " 'i': 9,\n",
       " 'search': 10,\n",
       " 'on': 11,\n",
       " 'in': 12,\n",
       " 'hope': 13,\n",
       " 'from': 14,\n",
       " 'whole': 15,\n",
       " 'we': 16,\n",
       " 'crumble': 17,\n",
       " 'forever': 18,\n",
       " 'lost': 19,\n",
       " 'to': 20,\n",
       " 'chaos': 21,\n",
       " 'never': 22,\n",
       " 'one': 23,\n",
       " 'again': 24,\n",
       " 'indistinctiveness': 25,\n",
       " 'immeasurability': 26,\n",
       " 'capitalism': 27,\n",
       " 'internet': 28,\n",
       " 'down': 29,\n",
       " 'obligations': 30,\n",
       " 'go': 31,\n",
       " 'byebye': 32,\n",
       " 'office': 33,\n",
       " 'rejoices': 34,\n",
       " 'cotton': 35,\n",
       " 'my': 36,\n",
       " 'mouth': 37,\n",
       " 'needles': 38,\n",
       " 'blood': 39,\n",
       " 'and': 40,\n",
       " 'bones': 41,\n",
       " 'hammers': 42,\n",
       " 'head': 43,\n",
       " 'mighty': 44,\n",
       " 'hummingbird': 45,\n",
       " 'drinks': 46,\n",
       " 'a': 47,\n",
       " 'grapefruits': 48,\n",
       " 'blossom': 49,\n",
       " 'blots': 50,\n",
       " 'out': 51,\n",
       " 'an': 52,\n",
       " 'airplane': 53,\n",
       " 'downvotes': 54,\n",
       " 'fall': 55,\n",
       " 'as': 56,\n",
       " 'sharp': 57,\n",
       " 'snowflakes': 58,\n",
       " 'of': 59,\n",
       " 'early': 60,\n",
       " 'winter': 61,\n",
       " 'execution': 62,\n",
       " 'seven': 63,\n",
       " 'ships': 64,\n",
       " 'tonight': 65,\n",
       " 'guess': 66,\n",
       " 'shouldve': 67,\n",
       " 'said': 68,\n",
       " 'goodbye': 69,\n",
       " 'saw': 70,\n",
       " 'eight': 71,\n",
       " 'this': 72,\n",
       " 'morning': 73,\n",
       " 'big': 74,\n",
       " 'words': 75,\n",
       " 'are': 76,\n",
       " 'so': 77,\n",
       " 'bad': 78,\n",
       " 'they': 79,\n",
       " 'can': 80,\n",
       " 'ruin': 81,\n",
       " 'haiku': 82,\n",
       " 'refrigerator': 83,\n",
       " 'at': 84,\n",
       " 'the': 85,\n",
       " 'end': 86,\n",
       " 'life': 87,\n",
       " 'kings': 88,\n",
       " 'queens': 89,\n",
       " 'pawns': 90,\n",
       " 'same': 91,\n",
       " 'box': 92,\n",
       " 'free': 93,\n",
       " 'mans': 94,\n",
       " 'short': 95,\n",
       " 'success': 96,\n",
       " 'measured': 97,\n",
       " 'money': 98,\n",
       " 'all': 99,\n",
       " 'work': 100,\n",
       " 'no': 101,\n",
       " 'play': 102,\n",
       " 'like': 103,\n",
       " 'lemmings': 104,\n",
       " 'off': 105,\n",
       " 'cliffs': 106,\n",
       " 'humans': 107,\n",
       " 'flock': 108,\n",
       " 'leader': 109,\n",
       " 'who': 110,\n",
       " 'just': 111,\n",
       " 'blind': 112,\n",
       " 'you': 113,\n",
       " 'left': 114,\n",
       " 'before': 115,\n",
       " 'woke': 116,\n",
       " 'beds': 117,\n",
       " 'empty': 118,\n",
       " 'toothbrush': 119,\n",
       " 'minty': 120,\n",
       " 'world': 121,\n",
       " 'keeps': 122,\n",
       " 'spinning': 123,\n",
       " 'nobody': 124,\n",
       " 'notices': 125,\n",
       " 'but': 126,\n",
       " 'what': 127,\n",
       " 'if': 128,\n",
       " 'it': 129,\n",
       " 'stopped': 130,\n",
       " 'live': 131,\n",
       " 'our': 132,\n",
       " 'lives': 133,\n",
       " 'make': 134,\n",
       " 'safely': 135,\n",
       " 'death': 136,\n",
       " 'why': 137,\n",
       " 'not': 138,\n",
       " 'take': 139,\n",
       " 'some': 140,\n",
       " 'risks': 141,\n",
       " 'endless': 142,\n",
       " 'beauty': 143,\n",
       " 'natures': 144,\n",
       " 'finest': 145,\n",
       " 'gift': 146,\n",
       " 'that': 147,\n",
       " 'girls': 148,\n",
       " 'shining': 149,\n",
       " 'grin': 150,\n",
       " 'has': 151,\n",
       " 'mantra': 152,\n",
       " 'am': 153,\n",
       " 'sad': 154,\n",
       " 'discover': 155,\n",
       " 'fun': 156,\n",
       " 'over': 157,\n",
       " 'zombies': 158,\n",
       " 'real': 159,\n",
       " 'will': 160,\n",
       " 'break': 161,\n",
       " 'your': 162,\n",
       " 'fucking': 163,\n",
       " 'foot': 164,\n",
       " 'gimps': 165,\n",
       " 'slow': 166,\n",
       " 'hell': 167,\n",
       " 'buildings': 168,\n",
       " 'remain': 169,\n",
       " 'monuments': 170,\n",
       " 'excess': 171,\n",
       " 'be': 172,\n",
       " 'gone': 173,\n",
       " 'last': 174,\n",
       " 'drop': 175,\n",
       " 'oil': 176,\n",
       " 'burnt': 177,\n",
       " 'by': 178,\n",
       " 'tank': 179,\n",
       " 'war': 180,\n",
       " 'autumn': 181,\n",
       " 'chill': 182,\n",
       " 'descends': 183,\n",
       " 'electric': 184,\n",
       " 'blanket': 185,\n",
       " 'returns': 186,\n",
       " 'snug': 187,\n",
       " 'cocoon': 188,\n",
       " 'sociopath': 189,\n",
       " 'pee': 190,\n",
       " 'trickle': 191,\n",
       " 'economics': 192,\n",
       " 'peer': 193,\n",
       " 'mirror': 194,\n",
       " 'fear': 195,\n",
       " 'face': 196,\n",
       " 'see': 197,\n",
       " 'broken': 198,\n",
       " 'yet': 199,\n",
       " 'beating': 200,\n",
       " 'stand': 201,\n",
       " 'wall': 202,\n",
       " 'looking': 203,\n",
       " 'up': 204,\n",
       " 'cracks': 205,\n",
       " 'for': 206,\n",
       " 'signs': 207,\n",
       " 'future': 208,\n",
       " 'love': 209,\n",
       " 'light': 210,\n",
       " 'darkened': 211,\n",
       " 'may': 212,\n",
       " 'rest': 213,\n",
       " 'peace': 214,\n",
       " 'telltale': 215,\n",
       " 'arrive': 216,\n",
       " 'temper': 217,\n",
       " 'sickness': 218,\n",
       " 'mornings': 219,\n",
       " 'hello': 220,\n",
       " 'mom': 221,\n",
       " 'dad': 222,\n",
       " 'tune': 223,\n",
       " 'xpost': 224,\n",
       " 'r': 225,\n",
       " 'video': 226,\n",
       " 'wouldnt': 227,\n",
       " 'mind': 228,\n",
       " 'while': 229,\n",
       " 'floating': 230,\n",
       " 'waves': 231,\n",
       " 'swallows': 232,\n",
       " 'saccharine': 233,\n",
       " 'pop': 234,\n",
       " 'spills': 235,\n",
       " 'car': 236,\n",
       " 'stereo': 237,\n",
       " 'its': 238,\n",
       " 'been': 239,\n",
       " 'long': 240,\n",
       " 'day': 241,\n",
       " 'mist': 242,\n",
       " 'fades': 243,\n",
       " 'time': 244,\n",
       " 'sun': 245,\n",
       " 'best': 246,\n",
       " 'gold': 247,\n",
       " 'her': 248,\n",
       " 'eyes': 249,\n",
       " 'kept': 250,\n",
       " 'westward': 251,\n",
       " 'city': 252,\n",
       " 'limits': 253,\n",
       " 'turn': 254,\n",
       " 'signal': 255,\n",
       " 'keep': 256,\n",
       " 'driving': 257,\n",
       " 'their': 258,\n",
       " 'kids': 259,\n",
       " 'could': 260,\n",
       " 'survive': 261,\n",
       " 'had': 262,\n",
       " 'stuff': 263,\n",
       " 'use': 264,\n",
       " 'wash': 265,\n",
       " 'suck': 266,\n",
       " 'haikus': 267,\n",
       " 'cant': 268,\n",
       " 'think': 269,\n",
       " 'anything': 270,\n",
       " 'nature': 271,\n",
       " 'roars': 272,\n",
       " 'die': 273,\n",
       " 'sheer': 274,\n",
       " 'terror': 275,\n",
       " 'those': 276,\n",
       " 'dont': 277,\n",
       " 'have': 278,\n",
       " 'ran': 279,\n",
       " 'better': 280,\n",
       " 'do': 281,\n",
       " 'or': 282,\n",
       " 'regret': 283,\n",
       " 'having': 284,\n",
       " 'done': 285,\n",
       " 'clouded': 286,\n",
       " 'tears': 287,\n",
       " 'seeing': 288,\n",
       " 'freshly': 289,\n",
       " 'dug': 290,\n",
       " 'grave': 291,\n",
       " 'beneath': 292,\n",
       " 'cover': 293,\n",
       " 'howl': 294,\n",
       " 'other': 295,\n",
       " 'poems': 296,\n",
       " 'coinsized': 297,\n",
       " 'spider': 298,\n",
       " 'birth': 299,\n",
       " 'moment': 300,\n",
       " 'well': 301,\n",
       " 'fails': 302,\n",
       " 'torches': 303,\n",
       " 'concrete': 304,\n",
       " 'caverns': 305,\n",
       " 'sky': 306,\n",
       " 'havent': 307,\n",
       " 'changed': 308,\n",
       " 'much': 309,\n",
       " 'upvotes': 310,\n",
       " 'increase': 311,\n",
       " 'wit': 312,\n",
       " 'comments': 313,\n",
       " 'decrease': 314,\n",
       " 'proportionately': 315,\n",
       " 'consolation': 316,\n",
       " 'prize': 317,\n",
       " 'goes': 318,\n",
       " 'ladies': 319,\n",
       " 'barbecue': 320,\n",
       " 'booth': 321,\n",
       " 'hooray': 322,\n",
       " 'vacancy': 323,\n",
       " 'heart': 324,\n",
       " 'hotel': 325,\n",
       " 'rented': 326,\n",
       " 'rooms': 327,\n",
       " 'pen': 328,\n",
       " 'lifted': 329,\n",
       " 'creations': 330,\n",
       " 'flow': 331,\n",
       " 'ceases': 332,\n",
       " 'ink': 333,\n",
       " 'meet': 334,\n",
       " 'bar': 335,\n",
       " 'bodies': 336,\n",
       " 'coalesce': 337,\n",
       " 'breakfast': 338,\n",
       " 'after': 339,\n",
       " 'dawn': 340,\n",
       " 'man': 341,\n",
       " 'paints': 342,\n",
       " 'streetlight': 343,\n",
       " 'lone': 344,\n",
       " 'star': 345,\n",
       " 'party': 346,\n",
       " 'bus': 347,\n",
       " 'austin': 348,\n",
       " 'exhausting': 349,\n",
       " 'put': 350,\n",
       " 'lime': 351,\n",
       " 'coconut': 352,\n",
       " 'fix': 353,\n",
       " 'belly': 354,\n",
       " 'ache': 355,\n",
       " '': 356,\n",
       " 'clanks': 357,\n",
       " 'coiled': 358,\n",
       " 'wire': 359,\n",
       " 'dodge': 360,\n",
       " 'deep': 361,\n",
       " 'holes': 362,\n",
       " 'iced': 363,\n",
       " 'pavement': 364,\n",
       " 'very': 365,\n",
       " 'blown': 366,\n",
       " 'shock': 367,\n",
       " 'sweat': 368,\n",
       " 'everyday': 369,\n",
       " 'focus': 370,\n",
       " 'rises': 371,\n",
       " 'above': 372,\n",
       " 'conquer': 373,\n",
       " 'everything': 374,\n",
       " 'sitting': 375,\n",
       " 'desk': 376,\n",
       " 'butt': 377,\n",
       " 'starting': 378,\n",
       " 'sleep': 379,\n",
       " 'doesnt': 380,\n",
       " 'snore': 381,\n",
       " 'stupid': 382,\n",
       " 'flattened': 383,\n",
       " 'tire': 384,\n",
       " 'curse': 385,\n",
       " 'frigid': 386,\n",
       " 'night': 387,\n",
       " 'wait': 388,\n",
       " 'caa': 389,\n",
       " 'op': 390,\n",
       " 'fag': 391,\n",
       " 'bro': 392,\n",
       " 'he': 393,\n",
       " 'even': 394,\n",
       " 'lift': 395,\n",
       " 'jimmies': 396,\n",
       " 'rustled': 397,\n",
       " 'making': 398,\n",
       " 'deal': 399,\n",
       " 'with': 400,\n",
       " 'mother': 401,\n",
       " 'fucker': 402,\n",
       " 'socks': 403,\n",
       " 'smell': 404,\n",
       " 'bleach': 405,\n",
       " 'laundry': 406,\n",
       " 'week': 407,\n",
       " 'filthy': 408,\n",
       " 'redtube': 409,\n",
       " 'chilling': 410,\n",
       " 'homies': 411,\n",
       " 'need': 412,\n",
       " 'watermelon': 413,\n",
       " 'now': 414,\n",
       " 'used': 415,\n",
       " 'welfare': 416,\n",
       " 'ornithology': 417,\n",
       " 'wave': 418,\n",
       " 'behaviors': 419,\n",
       " 'glorious': 420,\n",
       " 'howd': 421,\n",
       " 'spell': 422,\n",
       " 'uh': 423,\n",
       " 'gee': 424,\n",
       " 'thanks': 425,\n",
       " 'help': 426,\n",
       " 'electricity': 427,\n",
       " 'flashing': 428,\n",
       " 'across': 429,\n",
       " 'illuminating': 430,\n",
       " 'dream': 431,\n",
       " 'god': 432,\n",
       " 'know': 433,\n",
       " 'ending': 434,\n",
       " 'none': 435,\n",
       " 'hear': 436,\n",
       " 'golden': 437,\n",
       " 'flashes': 438,\n",
       " 'emptiness': 439,\n",
       " 'darkness': 440,\n",
       " 'consumes': 441,\n",
       " 'shines': 442,\n",
       " 'afar': 443,\n",
       " 'pain': 444,\n",
       " 'freedom': 445,\n",
       " 'happiness': 446,\n",
       " 'dead': 447,\n",
       " 'flies': 448,\n",
       " 'pleasure': 449,\n",
       " 'sacrifice': 450,\n",
       " 'rubber': 451,\n",
       " 'ducky': 452,\n",
       " 'bitch': 453,\n",
       " 'first': 454,\n",
       " 'bachelors': 455,\n",
       " 'then': 456,\n",
       " 'mba': 457,\n",
       " 'jd': 458,\n",
       " 'wheres': 459,\n",
       " 'job': 460,\n",
       " 'celebrities': 461,\n",
       " 'say': 462,\n",
       " 'realize': 463,\n",
       " 'heavens': 464,\n",
       " 'blue': 465,\n",
       " 'vault': 466,\n",
       " 'crowded': 467,\n",
       " 'opulent': 468,\n",
       " 'clouds': 469,\n",
       " 'kites': 470,\n",
       " 'kami': 471,\n",
       " 'bridge': 472,\n",
       " 'vast': 473,\n",
       " 'crossing': 474,\n",
       " 'gaps': 475,\n",
       " 'joining': 476,\n",
       " 'friends': 477,\n",
       " 'worldly': 478,\n",
       " 'within': 479,\n",
       " 'yay': 480,\n",
       " 'new': 481,\n",
       " 'subreddit': 482,\n",
       " 'funny': 483,\n",
       " 'damn': 484,\n",
       " 'dies': 485,\n",
       " 'eighty': 486,\n",
       " 'dropped': 487,\n",
       " 'soap': 488,\n",
       " 'prison': 489,\n",
       " 'shower': 490,\n",
       " 'was': 491,\n",
       " 'brutally': 492,\n",
       " 'stabbed': 493,\n",
       " 'migrating': 494,\n",
       " 'birds': 495,\n",
       " 'waters': 496,\n",
       " 'green': 497,\n",
       " 'sloped': 498,\n",
       " 'shore': 499,\n",
       " 'water': 500,\n",
       " 'treatment': 501,\n",
       " 'plant': 502,\n",
       " 'wrote': 503,\n",
       " 'didnt': 504,\n",
       " 'enjoy': 505,\n",
       " 'threw': 506,\n",
       " 'spherical': 507,\n",
       " 'shape': 508,\n",
       " 'suns': 509,\n",
       " 'bright': 510,\n",
       " 'essence': 511,\n",
       " 'contained': 512,\n",
       " 'white': 513,\n",
       " 'orb': 514,\n",
       " 'im': 515,\n",
       " 'around': 516,\n",
       " 'here': 517,\n",
       " 'seem': 518,\n",
       " 'nice': 519,\n",
       " 'people': 520,\n",
       " 'hard': 521,\n",
       " 'claw': 522,\n",
       " 'locked': 523,\n",
       " 'beak': 524,\n",
       " 'crushed': 525,\n",
       " 'landslide': 526,\n",
       " 'fighting': 527,\n",
       " '575': 528,\n",
       " 'peterson': 529,\n",
       " 'through': 530,\n",
       " '12': 531,\n",
       " '21': 532,\n",
       " 'kindle': 533,\n",
       " 'edition': 534,\n",
       " 'please': 535,\n",
       " 'share': 536,\n",
       " 'thoughts': 537,\n",
       " 'should': 538,\n",
       " 'afraid': 539,\n",
       " 'going': 540,\n",
       " 'back': 541,\n",
       " 'regularly': 542,\n",
       " 'scheduled': 543,\n",
       " 'programming': 544,\n",
       " 'walked': 545,\n",
       " 'field': 546,\n",
       " 'went': 547,\n",
       " 'along': 548,\n",
       " 'oh': 549,\n",
       " 'how': 550,\n",
       " 'dreams': 551,\n",
       " 'haunt': 552,\n",
       " 'show': 553,\n",
       " 'false': 554,\n",
       " 'reality': 555,\n",
       " 'lived': 556,\n",
       " 'fake': 557,\n",
       " 'parting': 558,\n",
       " 'ways': 559,\n",
       " 'bittersweet': 560,\n",
       " 'still': 561,\n",
       " 'beats': 562,\n",
       " 'confusing': 563,\n",
       " 'felt': 564,\n",
       " 'warm': 565,\n",
       " 'summer': 566,\n",
       " 'breeze': 567,\n",
       " 'sparkling': 568,\n",
       " 'air': 569,\n",
       " 'soul': 570,\n",
       " 'captured': 571,\n",
       " 'flight': 572,\n",
       " 'cloud': 573,\n",
       " 'two': 574,\n",
       " 'companions': 575,\n",
       " 'artists': 576,\n",
       " 'warcraft': 577,\n",
       " 'trick': 578,\n",
       " 'frozen': 579,\n",
       " 'running': 580,\n",
       " 'bye': 581,\n",
       " 'neverending': 582,\n",
       " 'kite': 583,\n",
       " 'rescued': 584,\n",
       " 'him': 585,\n",
       " 'she': 586,\n",
       " 'needs': 587,\n",
       " 'rescuing': 588,\n",
       " 'perhaps': 589,\n",
       " 'window': 590,\n",
       " 'fly': 591,\n",
       " 'paper': 592,\n",
       " 'sure': 593,\n",
       " 'covetous': 594,\n",
       " 'hands': 595,\n",
       " 'unhinge': 596,\n",
       " 'topmost': 597,\n",
       " 'button': 598,\n",
       " 'blouse': 599,\n",
       " 'sing': 600,\n",
       " 'song': 601,\n",
       " 'uplifting': 602,\n",
       " 'worlds': 603,\n",
       " 'falls': 604,\n",
       " 'silent': 605,\n",
       " 'snow': 606,\n",
       " 'canvas': 607,\n",
       " 'black': 608,\n",
       " 'birdsong': 609,\n",
       " 'bursting': 610,\n",
       " 'forth': 611,\n",
       " 'hotboxed': 612,\n",
       " 'fiveseater': 613,\n",
       " 'bullshitting': 614,\n",
       " 'stranger': 615,\n",
       " 'infatuation': 616,\n",
       " 'look': 617,\n",
       " 'wonders': 618,\n",
       " 'grateful': 619,\n",
       " 'today': 620,\n",
       " 'sexy': 621,\n",
       " 'spunky': 622,\n",
       " 'girl': 623,\n",
       " 'tolerates': 624,\n",
       " 'silliness': 625,\n",
       " 'means': 626,\n",
       " '3': 627,\n",
       " 'unwinds': 628,\n",
       " 'wonder': 629,\n",
       " 'youre': 630,\n",
       " 'stars': 631,\n",
       " '4i': 632,\n",
       " 'lie': 633,\n",
       " 'speechless': 634,\n",
       " 'gaze': 635,\n",
       " 'fixed': 636,\n",
       " 'upon': 637,\n",
       " 'beckons': 638,\n",
       " '5': 639,\n",
       " 'open': 640,\n",
       " 'away': 641,\n",
       " 'let': 642,\n",
       " 'honey': 643,\n",
       " 'bucket': 644,\n",
       " 'sits': 645,\n",
       " 'awaiting': 646,\n",
       " 'excrement': 647,\n",
       " 'falling': 648,\n",
       " 'rain': 649,\n",
       " 'brings': 650,\n",
       " 'social': 651,\n",
       " 'shyness': 652,\n",
       " 'where': 653,\n",
       " 'orpheus': 654,\n",
       " 'runs': 655,\n",
       " 'sober': 656,\n",
       " 'overcast': 657,\n",
       " 'clear': 658,\n",
       " 'stuck': 659,\n",
       " 'behind': 660,\n",
       " 'glass': 661,\n",
       " 'practice': 662,\n",
       " 'more': 663,\n",
       " 'become': 664,\n",
       " 'awesome': 665,\n",
       " 'would': 666,\n",
       " 'escape': 667,\n",
       " 'weiner': 668,\n",
       " 'pants': 669,\n",
       " 'case': 670,\n",
       " 'fire': 671,\n",
       " 'sleeping': 672,\n",
       " 'drug': 673,\n",
       " 'bed': 674,\n",
       " 'being': 675,\n",
       " 'dealer': 676,\n",
       " 'alarm': 677,\n",
       " 'law': 678,\n",
       " 'cooked': 679,\n",
       " 'pizza': 680,\n",
       " 'cut': 681,\n",
       " 'bastard': 682,\n",
       " 'into': 683,\n",
       " 'squares': 684,\n",
       " 'ate': 685,\n",
       " 'corners': 686,\n",
       " 'asks': 687,\n",
       " 'says': 688,\n",
       " 'stay': 689,\n",
       " 'delete': 690,\n",
       " 'number': 691,\n",
       " 'wedding': 692,\n",
       " 'bells': 693,\n",
       " 'ring': 694,\n",
       " 'dove': 695,\n",
       " 'released': 696,\n",
       " 'cage': 697,\n",
       " 'wilted': 698,\n",
       " 'chirp': 699,\n",
       " 'sweetly': 700,\n",
       " 'trees': 701,\n",
       " 'wish': 702,\n",
       " 'learn': 703,\n",
       " 'state': 704,\n",
       " 'knuckles': 705,\n",
       " 'tell': 706,\n",
       " 'lot': 707,\n",
       " 'menace': 708,\n",
       " 'south': 709,\n",
       " 'central': 710,\n",
       " 'drinking': 711,\n",
       " 'juice': 712,\n",
       " 'hood': 713,\n",
       " 'started': 714,\n",
       " 'five': 715,\n",
       " 'swiftly': 716,\n",
       " 'came': 717,\n",
       " 'curvy': 718,\n",
       " 'windy': 719,\n",
       " 'road': 720,\n",
       " 'seat': 721,\n",
       " 'belt': 722,\n",
       " 'against': 723,\n",
       " 'neck': 724,\n",
       " 'entombed': 725,\n",
       " 'thick': 726,\n",
       " 'whines': 727,\n",
       " 'does': 728,\n",
       " 'move': 729,\n",
       " 'icy': 730,\n",
       " 'write': 731,\n",
       " 'myself': 732,\n",
       " 'ponder': 733,\n",
       " 'petulance': 734,\n",
       " 'laserbeams': 735,\n",
       " 'called': 736,\n",
       " 'scifaiku': 737,\n",
       " 'were': 738,\n",
       " 'amazing': 739,\n",
       " 'seventy': 740,\n",
       " 'years': 741,\n",
       " 'give': 742,\n",
       " 'loves': 743,\n",
       " 'most': 744,\n",
       " 'embrace': 745,\n",
       " 'fits': 746,\n",
       " 'ever': 747,\n",
       " 'changing': 748,\n",
       " 'quickly': 749,\n",
       " 'word': 750,\n",
       " 'flowing': 751,\n",
       " 'valley': 752,\n",
       " 'koolaid': 753,\n",
       " 'yes': 754,\n",
       " 'farming': 755,\n",
       " 'harder': 756,\n",
       " 'than': 757,\n",
       " 'thought': 758,\n",
       " 'nemesis': 759,\n",
       " 'panini': 760,\n",
       " 'floor': 761,\n",
       " 'dogs': 762,\n",
       " 'happy': 763,\n",
       " 'least': 764,\n",
       " 'days': 765,\n",
       " 'beach': 766,\n",
       " 'sea': 767,\n",
       " 'grass': 768,\n",
       " 'mermaid': 769,\n",
       " 'born': 770,\n",
       " 'when': 771,\n",
       " 'kissed': 772,\n",
       " 'died': 773,\n",
       " 'whilst': 774,\n",
       " 'loved': 775,\n",
       " 'meow': 776,\n",
       " 'bark': 777,\n",
       " 'lasts': 778,\n",
       " 'souvenirs': 779,\n",
       " 'sand': 780,\n",
       " 'spines': 781,\n",
       " 'books': 782,\n",
       " 'shattered': 783,\n",
       " 'self': 784,\n",
       " 'esteem': 785,\n",
       " 'did': 786,\n",
       " 'get': 787,\n",
       " 'attached': 788,\n",
       " 'wrong': 789,\n",
       " 'drift': 790,\n",
       " 'far': 791,\n",
       " 'wading': 792,\n",
       " 'drought': 793,\n",
       " 'galaxy': 794,\n",
       " 'beyond': 795,\n",
       " 'horizons': 796,\n",
       " 'edge': 797,\n",
       " 'always': 798,\n",
       " 'reach': 799,\n",
       " 'obscure': 800,\n",
       " 'entity': 801,\n",
       " 'indicates': 802,\n",
       " 'existence': 803,\n",
       " 'consuming': 804,\n",
       " 'way': 805,\n",
       " 'high': 806,\n",
       " 'feel': 807,\n",
       " 'precious': 808,\n",
       " 'dumbass': 809,\n",
       " 'manager': 810,\n",
       " 'fixes': 811,\n",
       " 'wasted': 812,\n",
       " 'joy': 813,\n",
       " 'spring': 814,\n",
       " 'cometh': 815,\n",
       " 'sunset': 816,\n",
       " 'minute': 817,\n",
       " 'blog': 818,\n",
       " 'written': 819,\n",
       " 'waiting': 820,\n",
       " 'traffic': 821,\n",
       " 'markers': 822,\n",
       " 'special': 823,\n",
       " 'thing': 824,\n",
       " 'moon': 825,\n",
       " 'hides': 826,\n",
       " 'dark': 827,\n",
       " 'slaves': 828,\n",
       " 'bored': 829,\n",
       " 'reddit': 830,\n",
       " 'offers': 831,\n",
       " 'solace': 832,\n",
       " 'kittens': 833,\n",
       " 'titties': 834,\n",
       " 'captivated': 835,\n",
       " 'smile': 836,\n",
       " 'calls': 837,\n",
       " 'facebook': 838,\n",
       " 'list': 839,\n",
       " 'purge': 840,\n",
       " 'similar': 841,\n",
       " 'murder': 842,\n",
       " 'digital': 843,\n",
       " 'hatchet': 844,\n",
       " 'petal': 845,\n",
       " 'departs': 846,\n",
       " 'dancing': 847,\n",
       " 'patches': 848,\n",
       " 'wind': 849,\n",
       " 'cuts': 850,\n",
       " 'crab': 851,\n",
       " 'tuna': 852,\n",
       " 'salmon': 853,\n",
       " 'sweet': 854,\n",
       " 'potato': 855,\n",
       " 'waffle': 856,\n",
       " 'fries': 857,\n",
       " 'pinot': 858,\n",
       " 'grigio': 859,\n",
       " 'blissful': 860,\n",
       " 'misery': 861,\n",
       " 'beer': 862,\n",
       " 'cigarettes': 863,\n",
       " 'alone': 864,\n",
       " 'many': 865,\n",
       " 'vengeance': 866,\n",
       " 'sandwich': 867,\n",
       " 'eating': 868,\n",
       " 'pet': 869,\n",
       " 'truth': 870,\n",
       " 'wanted': 871,\n",
       " 'side': 872,\n",
       " 'deus': 873,\n",
       " 'ex': 874,\n",
       " 'machina': 875,\n",
       " 'pronouncing': 876,\n",
       " 'right': 877,\n",
       " 'only': 878,\n",
       " 'miss': 879,\n",
       " 'friendship': 880,\n",
       " 'meant': 881,\n",
       " 'sail': 882,\n",
       " 'docks': 883,\n",
       " 'squeezing': 884,\n",
       " 'agony': 885,\n",
       " 'splashes': 886,\n",
       " 'red': 887,\n",
       " 'faced': 888,\n",
       " 'relief': 889,\n",
       " 'starpricked': 890,\n",
       " 'past': 891,\n",
       " 'snowsalted': 892,\n",
       " 'woods': 893,\n",
       " 'clanging': 894,\n",
       " 'cabins': 895,\n",
       " 'remains': 896,\n",
       " 'point': 897,\n",
       " 'without': 898,\n",
       " 'tried': 899,\n",
       " 'guys': 900,\n",
       " 'chomp': 901,\n",
       " 'chew': 902,\n",
       " 'swallow': 903,\n",
       " 'silence': 904,\n",
       " 'heard': 905,\n",
       " 'gulp': 906,\n",
       " 'dissapoint': 907,\n",
       " 'finally': 908,\n",
       " 'cats': 909,\n",
       " 'id': 910,\n",
       " 'knead': 911,\n",
       " 'doughy': 912,\n",
       " 'butterface': 913,\n",
       " 'multigrain': 914,\n",
       " 'loaf': 915,\n",
       " 'touched': 916,\n",
       " 'breathing': 917,\n",
       " 'religious': 918,\n",
       " 'park': 919,\n",
       " 'want': 920,\n",
       " 'handicap': 921,\n",
       " 'unrequited': 922,\n",
       " 'really': 923,\n",
       " 'call': 924,\n",
       " 'lust': 925,\n",
       " 'emotionlessness': 926,\n",
       " 'void': 927,\n",
       " 'caused': 928,\n",
       " 'losing': 929,\n",
       " 'reflecting': 930,\n",
       " 'despair': 931,\n",
       " 'doing': 932,\n",
       " 'shaving': 933,\n",
       " 'leave': 934,\n",
       " 'mass': 935,\n",
       " 'pubes': 936,\n",
       " 'nasty': 937,\n",
       " 'fertile': 938,\n",
       " 'ground': 939,\n",
       " 'nope': 940,\n",
       " 'chuck': 941,\n",
       " 'testa': 942,\n",
       " 'soy': 943,\n",
       " 'un': 944,\n",
       " 'perdedor': 945,\n",
       " 'loser': 946,\n",
       " 'baby': 947,\n",
       " 'kill': 948,\n",
       " 'ideas': 949,\n",
       " 'overwhelming': 950,\n",
       " 'page': 951,\n",
       " 'shapes': 952,\n",
       " 'laying': 953,\n",
       " 'tread': 954,\n",
       " 'racing': 955,\n",
       " 'scared': 956,\n",
       " 'loyal': 957,\n",
       " 'land': 958,\n",
       " 'nor': 959,\n",
       " 'government': 960,\n",
       " 'milk': 961,\n",
       " 'sip': 962,\n",
       " 'cup': 963,\n",
       " 'lies': 964,\n",
       " 'lactose': 965,\n",
       " 'betrays': 966,\n",
       " 'held': 967,\n",
       " 'bear': 968,\n",
       " 'each': 969,\n",
       " 'others': 970,\n",
       " 'sharing': 971,\n",
       " 'shame': 972,\n",
       " 'remember': 973,\n",
       " 'ghost': 974,\n",
       " 'childhood': 975,\n",
       " 'friend': 976,\n",
       " 'monday': 977,\n",
       " 'sneaky': 978,\n",
       " 'weeping': 979,\n",
       " 'angel': 980,\n",
       " 'eye': 981,\n",
       " 'ill': 982,\n",
       " 'pass': 983,\n",
       " 'comfort': 984,\n",
       " 'leaves': 985,\n",
       " 'things': 986,\n",
       " 'balanced': 987,\n",
       " 'stones': 988,\n",
       " 'small': 989,\n",
       " 'orbs': 990,\n",
       " 'flickering': 991,\n",
       " 'moving': 992,\n",
       " 'dying': 993,\n",
       " 'dust': 994,\n",
       " 'solidarity': 995,\n",
       " 'painful': 996,\n",
       " 'express': 997,\n",
       " 'ship': 998,\n",
       " 'overnight': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>processed_title</th>\n",
       "      <th>ups</th>\n",
       "      <th>keywords</th>\n",
       "      <th>vectorized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1020ac</td>\n",
       "      <td>There's nothing inside / There is nothing outs...</td>\n",
       "      <td>5</td>\n",
       "      <td>[('inside', 0.5268), ('outside', 0.3751), ('se...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 2, 7, 8, 4, 9, 10, 11, 12, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>107cob</td>\n",
       "      <td>From whole we crumble / Forever lost to chaos ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[('chaos', 0.5962), ('crumble', 0.4749), ('for...</td>\n",
       "      <td>[14, 15, 16, 17, 4, 18, 19, 20, 21, 4, 22, 23,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>109a51</td>\n",
       "      <td>Indistinctiveness / Immeasurability / Capitalism</td>\n",
       "      <td>3</td>\n",
       "      <td>[('indistinctiveness', 0.7664), ('immeasurabil...</td>\n",
       "      <td>[25, 4, 26, 4, 27]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10eysi</td>\n",
       "      <td>Internet is down / Obligations go bye-bye / Of...</td>\n",
       "      <td>9</td>\n",
       "      <td>[('office', 0.5033), ('obligations', 0.4663), ...</td>\n",
       "      <td>[28, 6, 29, 4, 30, 31, 32, 4, 33, 34]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10f79k</td>\n",
       "      <td>Cotton in my mouth / Needles in my blood and b...</td>\n",
       "      <td>1</td>\n",
       "      <td>[('needles', 0.5314), ('cotton', 0.4806), ('bl...</td>\n",
       "      <td>[35, 12, 36, 37, 4, 38, 12, 36, 39, 40, 41, 4,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      id                                    processed_title  ups  \\\n",
       "0           0  1020ac  There's nothing inside / There is nothing outs...    5   \n",
       "1           1  107cob  From whole we crumble / Forever lost to chaos ...    1   \n",
       "2           2  109a51   Indistinctiveness / Immeasurability / Capitalism    3   \n",
       "3           3  10eysi  Internet is down / Obligations go bye-bye / Of...    9   \n",
       "4           4  10f79k  Cotton in my mouth / Needles in my blood and b...    1   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [('inside', 0.5268), ('outside', 0.3751), ('se...   \n",
       "1  [('chaos', 0.5962), ('crumble', 0.4749), ('for...   \n",
       "2  [('indistinctiveness', 0.7664), ('immeasurabil...   \n",
       "3  [('office', 0.5033), ('obligations', 0.4663), ...   \n",
       "4  [('needles', 0.5314), ('cotton', 0.4806), ('bl...   \n",
       "\n",
       "                                          vectorized  \n",
       "0  [1, 2, 3, 4, 5, 6, 2, 7, 8, 4, 9, 10, 11, 12, 13]  \n",
       "1  [14, 15, 16, 17, 4, 18, 19, 20, 21, 4, 22, 23,...  \n",
       "2                                 [25, 4, 26, 4, 27]  \n",
       "3              [28, 6, 29, 4, 30, 31, 32, 4, 33, 34]  \n",
       "4  [35, 12, 36, 37, 4, 38, 12, 36, 39, 40, 41, 4,...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"vectorized\"] = data[\"processed_title\"].apply(lambda x: [vocab_map[t] for t in tokenize(x)])\n",
    "data = data[[a.count(4) <= 2 for a in data['vectorized']]]\n",
    "max_length = find_max_length(data[\"vectorized\"])\n",
    "data = data[data['vectorized'].apply(lambda x: len(x) <= 19)]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,     2,     3, ...,     0,     0,     0],\n",
       "       [   14,    15,    16, ...,     0,     0,     0],\n",
       "       [   25,     4,    26, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  549,  5557,   424, ...,     0,     0,     0],\n",
       "       [ 1084,  6422,  1464, ...,     0,     0,     0],\n",
       "       [  414, 10429,     4, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_vectorized_list = data[\"vectorized\"].to_list()\n",
    "padded_data = pad_sequences(data_vectorized_list, padding='post')\n",
    "haikus = np.array(padded_data)\n",
    "haikus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([1, 2, 3], dtype=int32), 4),\n",
       " (array([2, 3, 4], dtype=int32), 5),\n",
       " (array([3, 4, 5], dtype=int32), 6),\n",
       " (array([4, 5, 6], dtype=int32), 2),\n",
       " (array([5, 6, 2], dtype=int32), 7),\n",
       " (array([6, 2, 7], dtype=int32), 8),\n",
       " (array([2, 7, 8], dtype=int32), 4),\n",
       " (array([7, 8, 4], dtype=int32), 9),\n",
       " (array([8, 4, 9], dtype=int32), 10),\n",
       " (array([ 4,  9, 10], dtype=int32), 11),\n",
       " (array([ 9, 10, 11], dtype=int32), 12),\n",
       " (array([10, 11, 12], dtype=int32), 13),\n",
       " (array([11, 12, 13], dtype=int32), 0),\n",
       " (array([12, 13,  0], dtype=int32), 0),\n",
       " (array([13,  0,  0], dtype=int32), 0),\n",
       " (array([0, 0, 0], dtype=int32), 0),\n",
       " (array([14, 15, 16], dtype=int32), 17),\n",
       " (array([15, 16, 17], dtype=int32), 4),\n",
       " (array([16, 17,  4], dtype=int32), 18),\n",
       " (array([17,  4, 18], dtype=int32), 19)]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_examples = []\n",
    "\n",
    "for haiku in haikus:\n",
    "    for i in range(len(haiku) - window_size + 1):\n",
    "        input_words = haiku[i:i+window_size-1]\n",
    "        output_word = haiku[i+window_size-1]\n",
    "        training_examples.append((input_words, output_word))\n",
    "training_examples[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense, SimpleRNN, GRU\n",
    "from sklearn.model_selection import train_test_split\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab_map)\n",
    "embedding_size = 128\n",
    "input_length = window_size - 1\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=input_length),\n",
    "    GRU(32),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def masked_loss(y_true, y_pred):\n",
    "    mask = K.cast(K.not_equal(y_true, 0), K.floatx())\n",
    "    loss = K.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    masked_loss = loss * mask\n",
    "    return K.sum(masked_loss) / K.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "model.compile(loss=masked_loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([example[0] for example in training_examples[:30000]])\n",
    "y = np.array([example[1] for example in training_examples[:30000]])\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "211/211 [==============================] - 17s 62ms/step - loss: 728.9174 - accuracy: 0.2820 - val_loss: 642.9690 - val_accuracy: 0.3087\n",
      "Epoch 2/10\n",
      "211/211 [==============================] - 12s 58ms/step - loss: 588.6475 - accuracy: 0.3342 - val_loss: 641.8521 - val_accuracy: 0.3370\n",
      "Epoch 3/10\n",
      "211/211 [==============================] - 12s 57ms/step - loss: 561.0453 - accuracy: 0.3591 - val_loss: 648.5424 - val_accuracy: 0.3447\n",
      "Epoch 4/10\n",
      "211/211 [==============================] - 12s 58ms/step - loss: 539.4976 - accuracy: 0.3770 - val_loss: 663.2150 - val_accuracy: 0.3440\n",
      "Epoch 5/10\n",
      "211/211 [==============================] - 12s 58ms/step - loss: 521.6893 - accuracy: 0.3855 - val_loss: 684.1929 - val_accuracy: 0.3420\n",
      "Epoch 6/10\n",
      "211/211 [==============================] - 12s 59ms/step - loss: 505.5494 - accuracy: 0.3927 - val_loss: 706.7286 - val_accuracy: 0.3457\n",
      "Epoch 7/10\n",
      "211/211 [==============================] - 12s 59ms/step - loss: 490.3127 - accuracy: 0.4001 - val_loss: 730.2386 - val_accuracy: 0.3327\n",
      "Epoch 8/10\n",
      "211/211 [==============================] - 12s 59ms/step - loss: 475.8141 - accuracy: 0.4064 - val_loss: 757.6943 - val_accuracy: 0.3343\n",
      "Epoch 9/10\n",
      "110/211 [==============>...............] - ETA: 5s - loss: 459.0900 - accuracy: 0.4114"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_map_inv = dict([(value, key) for key, value in vocab_map.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poem(input_words: list):\n",
    "    while len(input_words) < window_size - 1:\n",
    "        input_words.insert(0, \"<pad>\")\n",
    "    vectorized_input = [vocab_map[word] for word in input_words]\n",
    "    print(f\"User specified words {input_words} which were vectorized as {vectorized_input}\")\n",
    "    output_poem = input_words\n",
    "    \n",
    "    for i in range(19 - window_size - 1):\n",
    "        input = np.array(vectorized_input[i:i+window_size-1]).reshape((1, window_size-1))\n",
    "        prediction = np.array(model.predict(input, verbose=0))\n",
    "        # new_word_vector = (prediction[0].argsort()[::-1])[np.random.randint(0,1)]\n",
    "        new_word_vector = (prediction[0].argsort()[::-1])[0]\n",
    "        vectorized_input.append(new_word_vector)\n",
    "        new_word = vocab_map_inv[new_word_vector]\n",
    "        output_poem.append(new_word)\n",
    "    output = \" \".join(output_poem)\n",
    "    print(f\"OUTPUT POEM: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User specified words ['once', 'i'] which were vectorized as [1587, 9]\n",
      "OUTPUT POEM: once i have been a haiku / i am not a haiku / i am not a\n"
     ]
    }
   ],
   "source": [
    "generate_poem([\"once\", \"i\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User specified words ['college', 'students', 'are'] which were vectorized as [3571, 4015, 76]\n",
      "OUTPUT POEM: college students are <pad> the <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "generate_poem([\"college\", \"students\", \"are\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User specified words ['fun', 'today', 'and', 'tomorrow', 'together'] which were vectorized as [156, 620, 40, 1543, 2013]\n",
      "OUTPUT POEM: fun today and tomorrow together / the / to a worst been the bad / a same / i haiku\n"
     ]
    }
   ],
   "source": [
    "generate_poem([\"fun\", \"today\", \"and\", \"tomorrow\", \"together\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User specified words ['the', 'bear', 'and', 'i', 'will'] which were vectorized as [85, 968, 40, 9, 160]\n",
      "OUTPUT POEM: the bear and i will breaks flowers see be my / / <pad> life the and <pad> <pad> same fit\n"
     ]
    }
   ],
   "source": [
    "generate_poem([\"the\", \"bear\", \"and\", \"i\", \"will\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User specified words ['you', 'are', 'a', 'movie'] which were vectorized as [113, 76, 47, 4401]\n",
      "OUTPUT POEM: you are a movie not fool / / <pad> but and <pad> me not <pad> me / <pad> me\n"
     ]
    }
   ],
   "source": [
    "generate_poem([\"you\",\"are\", \"a\", \"movie\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_random_words(vocab_map, number):\n",
    "    vocabs = list(vocab_map.keys())\n",
    "    inds = list(np.random.randint(0, len(vocabs), number))\n",
    "    output_words = []\n",
    "    for i in inds:\n",
    "        output_words.append(vocabs[i])\n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User specified words ['sesquidecember', 'fairytale'] which were vectorized as [11301, 14997]\n",
      "OUTPUT POEM: sesquidecember fairytale / when are be <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "generate_poem(pick_random_words(vocab_map, window_size-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User specified words ['coulnt', 'rockefeller', 'cashmere', 'designing', 'weiner'] which were vectorized as [16218, 10467, 7594, 12341, 668]\n",
      "OUTPUT POEM: coulnt rockefeller cashmere designing weiner / but i just be things <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "generate_poem(pick_random_words(vocab_map, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User specified words ['hi', 'park', 'ball', 'stick', 'water'] which were vectorized as [4104, 919, 4036, 2121, 500]\n",
      "OUTPUT POEM: hi park ball stick water <pad> of snow / <pad> on / behind <pad> and happy in <pad> on /\n"
     ]
    }
   ],
   "source": [
    "generate_poem([\"hi\", \"park\", \"ball\", \"stick\", \"water\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci1470",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
