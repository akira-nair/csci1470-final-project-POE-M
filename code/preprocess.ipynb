{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/haiku.csv')\n",
    "data = data.replace(\"/\", \" / \", regex=True)\n",
    "data = data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence: str):\n",
    "    tokens = sentence.lower().split()\n",
    "    tokens = [process_token(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "def process_token(token: str):\n",
    "    if token.strip() == \"/\":\n",
    "        return token\n",
    "    return re.sub(r'[^\\w\\s]', '', token.strip())\n",
    "\n",
    "def vectorize(tokens):\n",
    "    vocab, index = {}, 1\n",
    "    vocab['<pad>'] = 0\n",
    "    for token in tokens:\n",
    "        token = token.strip()\n",
    "        if token not in vocab:\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab\n",
    "\n",
    "def find_max_length(vectorized_poems):\n",
    "    max_length = 0\n",
    "    for poem in vectorized_poems:\n",
    "        max_length = max(max_length, len(poem))   \n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \" \".join(data[\"processed_title\"].to_list())\n",
    "tokens = tokenize(all_text)\n",
    "vocab_map = vectorize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"vectorized\"] = data[\"processed_title\"].apply(lambda x: [vocab_map[t] for t in tokenize(x)])\n",
    "data = data[[a.count(4) <= 2 for a in data['vectorized']]]\n",
    "max_length = find_max_length(data[\"vectorized\"])\n",
    "data = data[data['vectorized'].apply(lambda x: len(x) <= 19)]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vectorized_list = data[\"vectorized\"].to_list()\n",
    "padded_data = pad_sequences(data_vectorized_list, padding='post')\n",
    "haikus = np.array(padded_data)\n",
    "haikus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = []\n",
    "\n",
    "for haiku in haikus:\n",
    "    for i in range(len(haiku) - window_size + 1):\n",
    "        input_words = haiku[i:i+window_size-1]\n",
    "        output_word = haiku[i+window_size-1]\n",
    "        training_examples.append((input_words, output_word))\n",
    "training_examples[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense, SimpleRNN, GRU\n",
    "from sklearn.model_selection import train_test_split\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab_map)\n",
    "embedding_size = 128\n",
    "input_length = window_size - 1\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=input_length),\n",
    "    GRU(32),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def masked_loss(y_true, y_pred):\n",
    "    mask = K.cast(K.not_equal(y_true, 0), K.floatx())\n",
    "    loss = K.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    masked_loss = loss * mask\n",
    "    return K.sum(masked_loss) / K.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "model.compile(loss=masked_loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([example[0] for example in training_examples[:15000]])\n",
    "y = np.array([example[1] for example in training_examples[:15000]])\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_map_inv = dict([(value, key) for key, value in vocab_map.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poem(input_words: list):\n",
    "    while len(input_words) < window_size - 1:\n",
    "        input_words.insert(0, \"<pad>\")\n",
    "    vectorized_input = [vocab_map[word] for word in input_words]\n",
    "    print(f\"User specified words {input_words} which were vectorized as {vectorized_input}\")\n",
    "    output_poem = input_words\n",
    "    \n",
    "    for i in range(19 - window_size - 1):\n",
    "        input = np.array(vectorized_input[i:i+window_size-1]).reshape((1, window_size-1))\n",
    "        prediction = np.array(model.predict(input, verbose=0))\n",
    "        # new_word_vector = (prediction[0].argsort()[::-1])[np.random.randint(0,1)]\n",
    "        new_word_vector = (prediction[0].argsort()[::-1])[0]\n",
    "        vectorized_input.append(new_word_vector)\n",
    "        new_word = vocab_map_inv[new_word_vector]\n",
    "        output_poem.append(new_word)\n",
    "    output = \" \".join(output_poem)\n",
    "    print(f\"OUTPUT POEM: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_poem([\"once\", \"i\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_poem([\"college\", \"students\", \"are\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_poem([\"fun\", \"today\", \"and\", \"tomorrow\", \"together\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_poem([\"the\", \"bear\", \"and\", \"i\", \"will\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_poem([\"you\",\"are\", \"a\", \"movie\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_random_words(vocab_map, number):\n",
    "    vocabs = list(vocab_map.keys())\n",
    "    inds = list(np.random.randint(0, len(vocabs), number))\n",
    "    output_words = []\n",
    "    for i in inds:\n",
    "        output_words.append(vocabs[i])\n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_poem(pick_random_words(vocab_map, window_size-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_poem(pick_random_words(vocab_map, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_poem([\"hi\", \"park\", \"ball\", \"stick\", \"water\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci1470",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
